{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scikic API v2.0\n",
    " \n",
    "## Overview\n",
    "\n",
    "The scikic api is an inference tool which takes a set of question/answer items and then queries a series of local and remote databases to generate conditional probability distributions over various features. The api is highly modular, and some modules don't use this probabilistic framework, for example the music module simply contacts api.bandsintown.com to provide useful suggestions about local bands to go and see.\n",
    "\n",
    "The conditional probabilities are combined using a Bayesian network, using the pyMC module. Each module can provide pyMC 'features' which create functions to output the relevant probability distributions.\n",
    "\n",
    "## Question/Answer dictionaries\n",
    "\n",
    "The questions and answers are organised to be in 4 value tuples, containing:\n",
    "\n",
    "- dataset: lets the system know which class to instantiate etc, examples: postcode, census, movielens, ...etc\n",
    "- dataitem: used by classes to know which aspect of the dataset. For example in the movielens dataset, one could be interested in whether the user's seen a film or what rating they've given the film.\n",
    "- detail: often unused by the classes, could be, for example, the id of the film we want to know about.\n",
    "- answer: the user's answer.\n",
    "\n",
    "## Future Changes/TODO list\n",
    "\n",
    "Heads-up about a couple of changes:\n",
    "\n",
    " - ~~Vasily suggested I move the 'action' selection out and make it part of the URL, to make it more RESTful.~~ DONE.\n",
    " - ~~I'll be combining the API query that gets the question string, with the one that generates the 'raw' question tuple. Originally the API was designed when it was run on the same server as the code that was querying it, so extra API calls didn't cost much. As the calls will now be remote, they need combining.~~ DONE\n",
    " - ~~document the new API~~ DONE (below)\n",
    " - fix security issues (stop outputting debug info to client)\n",
    " - add useful error messages to API's output\n",
    " - incorporate Cambridge's code\n",
    " - write a new install script for the setup\n",
    " - install on front end\n",
    " - The version number will be used in future.\n",
    " - The API doesn't currently check the key. Obviously this will be implemented in future.\n",
    " - Get the IP->Location working again (API change?)\n",
    " - Check load balancer to backend API still working (this shouldn't be need altering)\n",
    " - Set up SSL for API load balancer (previously left connection between frontend and backend non-SSL as it's all in the AWS datacentre).\n",
    "\n",
    "## Available actions\n",
    "\n",
    "Here are some examples of the API in action. A **POST** request is used for the query, in case the data we're sending is too large to fit in a GET request. The POST body is json.\n",
    "\n",
    "### 1. question\n",
    "\n",
    "#### Parameters\n",
    "One passes to this call in data, a dictionary containing:\n",
    " - 'questions_asked'\n",
    " - 'unprocessed_questions'\n",
    " - 'facts'\n",
    " - 'target'\n",
    "\n",
    "The 'questions_asked' include all the questions we've asked, so we don't ask the same question again.\n",
    "The 'facts' dictionary contains information that we've found from earlier questions, etc. It allows caching of the calculations from earlier calls to the API.\n",
    "The 'unprocessed_questions' are a list of question/answers that we've asked before, that haven't had their results added to the 'facts' dictionary.\n",
    "The 'target' item is currently unused, but in the future will allow the choice of question to be selected to maximise the information about a particular feature.\n",
    "\n",
    "#### Returns\n",
    "\n",
    "This call returns a dictionary containing three things:\n",
    "\n",
    " - a 'facts' dictionary - this you can pass back in future so that the method doesn't have to recalculate or generate earlier results.\n",
    " - a 'question' dictionary, containing the dictionary describing the question, e.g. {dataset,dataitem,detail}.\n",
    " - a 'question_string' dictionary, containing:\n",
    " \n",
    "    - 'text' - the actual string of the question (e.g. \"Who's your favourite band or artist?\")\n",
    "    - 'type' - the type of question (it might just want a text reply, so this would equal 'text' or it might be a choice, and so would say 'select'\n",
    "    - 'options' - optional, and is included if the type is 'select'.\n",
    "\n",
    "\n",
    "#### Usage example\n",
    "\n",
    "1. One might initially call this method with all these fields being empty. The method will return an empty 'facts' dictionary and a question dictionary for the first question you want to ask. \n",
    "2. Once you have an answer from the user you would call the method again, this time with the question/answer tuple as both 'questions_asked' and 'unprocessed_questions'.\n",
    "3. The method will return a facts dictionary now, potentially with some results from the processing of the last answer, and another question for you to ask. \n",
    "4. When you call the method a third time (with the user's second answer), you'll pass all the question/answer tuples that you've asked so far in 'questions_asked' and the last question/answer tuple in 'unprocessed_questions'. You'll also pass the new facts dictionary, that now has some content in it.\n",
    "5. This process continues, with the facts dictionary growing each time, the 'questions_asked' growing too, and each time you just have one item in 'unprocessed_questions'.\n",
    "\n",
    "To summarise:\n",
    "\n",
    "Generating a question requires a dictionary of 'questions_asked', 'facts' and 'target'. The 'questions_asked' is a list of dictionaries of previous questions, that you want to avoid asking again.\n",
    "The 'unprocessed_questions' are questions that you've asked already and that haven't been incorporated into the 'facts' dictionary.\n",
    "\n",
    "Below is an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#The url of the API currently (in the long run we should give the backend a domain, maybe api.scikic.org?)\n",
    "#apiurl = 'http://api.scikic.org' #production api\n",
    "apiurl = 'http://dev.scikic.org' #development api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"facts\": {\"guess_loc\": {}, \"where\": {}}, \"question_string\": {\"type\": \"select\", \"question\": \"Which country are you in now?\", \"options\": [\"United States\", \"United Kingdom\", \"Germany\", \"Canada\", \"France\", \"Other\"]}, \"question\": {\"detail\": \"{}\", \"dataitem\": \"country\", \"dataset\": \"geoloc\"}}'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "#We provide data about previous questions etc:\n",
    "#data consists of a dictionary of (all of these are optional):\n",
    "\n",
    "#'questions_asked': An array of previous questions and answers we've asked, consists of a list of dictionaries.\n",
    "questions_asked = [{'dataset':'postal','dataitem':'postcode','detail':''},{\"detail\": \"\", \"dataitem\": \"favourite_artist\", \"dataset\": \"music\"}]\n",
    "\n",
    "#none of the questions that have been asked before have been processed.\n",
    "unprocessed_questions = questions_asked\n",
    "\n",
    "#'facts': If you've run the inference query and stored a copy of the facts dictionary you can pass it back.\n",
    "#this makes it quicker. In this case we've not yet had any questions processed.\n",
    "facts = {}\n",
    "\n",
    "#'target': What feature we want to know more about (example: 'age', 'gender', 'location') NOT YET IMPLEMENTED\n",
    "#not used. All these items are optional, so we just don't include it.\n",
    "\n",
    "#Build the dictionary:\n",
    "data = {'unprocessed_questions':unprocessed_questions,'questions_asked':questions_asked,'facts':facts}\n",
    "\n",
    "#put it into the payload of the request. This also includes the version, the api key we're using and the action we want (in this case generate a question)\n",
    "payload = {\"version\":1, 'data': data, 'apikey': 'YOUR_API_KEY_HERE', 'action':'question'}\n",
    "r = requests.post(apiurl+'/question',json=payload)\n",
    "r.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here it has output:\n",
    "\n",
    " - the facts dictionary: Empty\n",
    " - the question it suggests we ask, which in an earlier case is from the movielens dataset class, and is asking if they've seen movie number 2541.\n",
    " - the question string, with the type of question and options (if relevant). For example the dictionary `{'dataset':'postal','dataitem':'postcode','detail':''}` gets converted to the question \"`What's your postcode?`\" with type \"`text`\" (i.e. the user can type anything)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. inference\n",
    "\n",
    "#### Parameters\n",
    "\n",
    "The data dictionary should contain three things, similar to the \"action: question\" above,\n",
    "\n",
    "- questions_asked - list of question tuples that we've asked (with their answers)\n",
    "- unprocessed_questions - list of question tuples that we've asked (with their answers), that have not yet been added to the facts dictionary.\n",
    "- facts - the current 'facts' dictionary (possibly provided by earlier calls using action:question)\n",
    "\n",
    "Returns a dictionary of:\n",
    "\n",
    "##### features\n",
    "This is a dictionary of things that have probabilities associated, for example one of its items is 'household' with the following fields:\n",
    " \n",
    "     {\"distribution\": [0.029, 0.058, 0.23, 0.034, 0.070, 0.026, 0.055, 0.036, 0.14, 0.024, 0.023, 0.035, 0.24], \"quartiles\": {\"upper\": 11, \"lower\": 2, \"mean\": 6.46}}\n",
    " \n",
    " where the distribution is how likely the person is to be in each of the categories of a household (these categories can be found in the metadata from the module, or elsewhere). The quartiles don't mean much here as this is properly categorical data. This makes more sense in data such as age.\n",
    " \n",
    "##### facts\n",
    "As mentioned previously is a set of truths about the user generated from processing their answers.\n",
    "\n",
    "##### Insights\n",
    "This is a list of strings, generated by each module, here is an example:\n",
    " \n",
    "     [\"I can\\'t tell which country you\\'re in, just looking at your facebook likes, as I can\\'t see your facebook likes!\", \"You are aged between 20 and 33.\", \"You don\\'t have children living at home\", \" I think you are Christian or of no religion.\"]}\n",
    " \n",
    "Note regarding the distribution above: If some probabilities are zero towards the end of a list then the list will be truncated. For example if inference is certain the user is a male, then the output list will be {\"factor_gender\":[1.0]}. If they are definitely female it will be {\"factor_gender\":[0.0, 1.0]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example call using action:inference\n",
    "\n",
    "Below we set up the data dictionary with one question asked (and answered) regarding postcode. It also contains the same question/answer dictionary as an unprocessed question. The facts dictionary is empty.\n",
    "\n",
    "The output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==Facts==\n",
      "{u'guess_loc': {}, u'first_name': u'Alan', u'where': {u'ukcensus': [{u'item': u'E00172420', u'probability': 1.0, u'level': u'oa'}], u'latlong': [{u'item': [53.389942, -1.476789], u'probability': 1.0}], u'country': [{u'item': u'gb', u'probability': 1.0}], u'city': [{u'item': [u'Sheffield', u'uk'], u'probability': 1.0}]}, u'where_history': {u'error': u'no_fb_likes'}}\n",
      "\n",
      "==Features==\n",
      "religion\n",
      "{u'distribution': [0.24977777777777777, 0.03955555555555555, 0.043111111111111114, 0.035555555555555556, 0.04844444444444444, 0.04488888888888889, 0.03688888888888889, 0.5017777777777778], u'quartiles': {u'upper': 7, u'lower': 1, u'mean': 4.384444444444444}}\n",
      "household\n",
      "{u'distribution': [0.03111111111111111, 0.028888888888888888, 0.19777777777777777, 0.033777777777777775, 0.027555555555555555, 0.03777777777777778, 0.043555555555555556, 0.049777777777777775, 0.29333333333333333, 0.032, 0.024888888888888887, 0.04133333333333333, 0.1582222222222222], u'quartiles': {u'upper': 9, u'lower': 2, u'mean': 6.671555555555555}}\n",
      "oa\n",
      "{u'distribution': [1.0], u'quartiles': {u'upper': 0, u'lower': 0, u'mean': 0.0}}\n",
      "factor_gender\n",
      "{u'distribution': [1.0], u'quartiles': {u'upper': 0, u'lower': 0, u'mean': 0.0}}\n",
      "factor_age\n",
      "{u'distribution': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.012, 0.058666666666666666, 0.026222222222222223, 0.03111111111111111, 0.034666666666666665, 0.035555555555555556, 0.03688888888888889, 0.028888888888888888, 0.02711111111111111, 0.018666666666666668, 0.06355555555555556, 0.008444444444444444, 0.03955555555555555, 0.030222222222222223, 0.030222222222222223, 0.013333333333333334, 0.026222222222222223, 0.006222222222222222, 0.024888888888888887, 0.006666666666666667, 0.008, 0.017777777777777778, 0.0, 0.03422222222222222, 0.022222222222222223, 0.004888888888888889, 0.035111111111111114, 0.010222222222222223, 0.015111111111111112, 0.0, 0.056, 0.015111111111111112, 0.0, 0.0, 0.00044444444444444447, 0.05555555555555555, 0.0, 0.0, 0.018222222222222223, 0.018666666666666668, 0.0, 0.019555555555555555, 0.017333333333333333, 0.0, 0.018222222222222223, 0.0, 0.0, 0.0, 0.030222222222222223, 0.0, 0.024444444444444446, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.019555555555555555], u'quartiles': {u'upper': 51, u'lower': 28, u'mean': 40.981777777777765}}\n",
      "\n",
      "==Text insights==\n",
      "[u\"Note: I can't see your facebook likes.\", u\"You're called Alan\", u'People with your name are mostly aged between 39 and 86', u'Your name was most popular in about 1915', u'OSM Answer system online', u'There are 11 museums in your town or area.', u'You are aged between 28 and 51.', u'You are male', u\"You don't have children living at home\", u' I think you are Christian or of no religion.', u'Your neighbourhood has a population density of 1930 people per square kilometre, 4.7 times the average for England.', u'Half the people in your neighbourhood are younger than 25 years old.', u'People in your area are 3.0 times more likely to go to work on foot than the national average.']\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "postcode = 's63af'\n",
    "questions_asked = []\n",
    "\n",
    "#their address:\n",
    "questions_asked.append({'dataset':'postal','dataitem':'postcode','detail':'','answer':'s63af'})\n",
    "\n",
    "#Additional info from facebook:\n",
    "fb_data = json.dumps({'reply':json.dumps({'first_name':'Alan'})})\n",
    "questions_asked.append({'dataset':'facebook','dataitem':'','detail':'','answer':fb_data})\n",
    "\n",
    "#we assume that we've not processed any of these questions yet. \n",
    "#You'll only not do this if you've asked some of these already,\n",
    "#and populated the 'facts' dictionary.\n",
    "unprocessed_questions = questions_asked[:]\n",
    "facts = {}\n",
    "data = {'questions_asked':questions_asked,'unprocessed_questions':unprocessed_questions,'facts':facts}\n",
    "payload = {\"version\":1, 'data': data, 'apikey': 'YOUR_API_KEY_HERE'}\n",
    "r = requests.post(apiurl+'/inference',json=payload)\n",
    "#print r.content\n",
    "result = json.loads(r.content)\n",
    "print \"==Facts==\"\n",
    "print result['facts']\n",
    "print \"\"\n",
    "print \"==Features==\"\n",
    "for feat in result['features']:\n",
    "    print feat\n",
    "    print result['features'][feat]\n",
    "print \"\"\n",
    "print \"==Text insights==\"\n",
    "print result['insights']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the example above one can see that after just processing the answer about postcode, quite a bit of new info has been generated.\n",
    "\n",
    "First the facts dictionary. This is often dataset specific stuff, although I've tried to make things compatable between datasets.\n",
    "\n",
    " - 'guess_loc': {} - info on whether it guessed the location of the user from IP address (I think)\n",
    " - 'where': - the dictionary describing the location of the user's home. This is quite tricky, as different sources of data have different resolutions, etc about this.\n",
    "     - {'ukcensus': [{'item': 'E00172420', 'probability': 1.0, 'level': 'oa'}] - In terms of the UK census, with have a list of output areas for this person's home. This list only has one item in it. Each item in the list has a probability associated, in this case the probability is 1.0: We are certain the person is in that output area.\n",
    "     - 'country': [{'item': 'gb', 'probability': 1.0}] - Which country they're in. A list of countries with associated probabilities.\n",
    "     - 'city': [{'item': ['Sheffield', 'uk'], 'probability': 1.0}]} - which city their in (with probabilites).\n",
    "     - 'where_history': {u'error': u'no_fb_likes'}} - if we have access to facebook likes, it tries to generate a history of where the person's lived. But the error item means that it's not managed to get hold of the likes to do this.\n",
    "\n",
    "Next the features dictionary. Different datasets provide different conditional probability distributions. Each distribution has at least two features associated. If they are not in the list already the module adds them, thus one has a list of features at the end.\n",
    "\n",
    "The value of these features is then estimated using MCMC with the pyMC module.\n",
    "\n",
    "The example above has five features: religion, household, (census) output area, gender and age. They're all catagorical (for now) although that's purely due to the type of data that we have about them.\n",
    "\n",
    "Looking just at the household feature: [0.022222222222222223, 0.05688888888888889, 0.23466666666666666, 0.03244444444444444, 0.08266666666666667, 0.028, 0.059111111111111114, 0.03955555555555555, 0.14222222222222222, 0.024, 0.024444444444444446, 0.028444444444444446, 0.22533333333333333]\n",
    "\n",
    "Each value refers to the probability of being of a given type of household (the labels associated are available via the metadata of the module).\n",
    "\n",
    "Finally there are insights, these are simple strings of facts about the person:\n",
    " - \"I can't tell which country you're in, just looking at your facebook likes, as I can't see your facebook likes!\" - this first one is more of an error message warning us that we couldn't get to their facebook like data.\n",
    " - \"You are aged between 19 and 31\"\n",
    " - \"I think you are Christian or of no religion\"\n",
    " \n",
    "Here's another example, with an american zip code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"facts\": {\"guess_loc\": {}, \"where\": {}, \"where_history\": {\"error\": \"no_fb_likes\"}}, \"features\": {\"factor_gender\": {\"distribution\": [0.3337777777777778, 0.6662222222222223], \"quartiles\": {\"upper\": 1, \"lower\": 0, \"mean\": 0.6662222222222223}}, \"factor_age\": {\"distribution\": [0.003111111111111111, 0.0071111111111111115, 0.012, 0.008444444444444444, 0.012888888888888889, 0.008444444444444444, 0.0057777777777777775, 0.014666666666666666, 0.008888888888888889, 0.014666666666666666, 0.010222222222222223, 0.015111111111111112, 0.010222222222222223, 0.009777777777777778, 0.008, 0.008888888888888889, 0.013777777777777778, 0.010666666666666666, 0.010666666666666666, 0.010666666666666666, 0.011555555555555555, 0.008888888888888889, 0.012888888888888889, 0.014666666666666666, 0.011111111111111112, 0.010666666666666666, 0.008444444444444444, 0.011555555555555555, 0.008, 0.010666666666666666, 0.013777777777777778, 0.008444444444444444, 0.011555555555555555, 0.008888888888888889, 0.007555555555555556, 0.009333333333333334, 0.010666666666666666, 0.009777777777777778, 0.008444444444444444, 0.014666666666666666, 0.009333333333333334, 0.010222222222222223, 0.006666666666666667, 0.008888888888888889, 0.006222222222222222, 0.008, 0.007555555555555556, 0.009777777777777778, 0.008444444444444444, 0.009777777777777778, 0.010666666666666666, 0.012, 0.009777777777777778, 0.008888888888888889, 0.007555555555555556, 0.009777777777777778, 0.008888888888888889, 0.010222222222222223, 0.008888888888888889, 0.008888888888888889, 0.009333333333333334, 0.006666666666666667, 0.011555555555555555, 0.009777777777777778, 0.007555555555555556, 0.009777777777777778, 0.0071111111111111115, 0.008, 0.011555555555555555, 0.008888888888888889, 0.008444444444444444, 0.008444444444444444, 0.0071111111111111115, 0.010222222222222223, 0.011111111111111112, 0.012888888888888889, 0.010222222222222223, 0.011555555555555555, 0.009777777777777778, 0.011555555555555555, 0.008888888888888889, 0.009333333333333334, 0.008, 0.011555555555555555, 0.011555555555555555, 0.012, 0.008888888888888889, 0.011111111111111112, 0.009777777777777778, 0.009777777777777778, 0.011111111111111112, 0.011111111111111112, 0.009777777777777778, 0.006666666666666667, 0.008888888888888889, 0.008, 0.009333333333333334, 0.012444444444444444, 0.010222222222222223, 0.008888888888888889, 0.015111111111111112], \"quartiles\": {\"upper\": 76, \"lower\": 23, \"mean\": 49.77866666666667}}}, \"insights\": [\"Note: I can\\'t see your facebook likes.\", \"OSM Answer system online\"]}'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "questions_asked = [{'dataset':'postal','dataitem':'zipcode','detail':'','answer':'86021'}]\n",
    "unprocessed_questions = [{'dataset':'postal','dataitem':'zipcode','detail':'','answer':'86021'}]\n",
    "facts = {}\n",
    "        \n",
    "data = {'questions_asked':questions_asked,'unprocessed_questions':unprocessed_questions,'facts':facts}\n",
    "\n",
    "payload = {\"version\":1, 'data': data, 'apikey': 'YOUR_API_KEY_HERE'}\n",
    "r = requests.post(apiurl+'/inference',json=payload)\n",
    "r.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see this is similar, except the 'where' item in the dictionary has a 'uscensus' item within it. This contains two items:\n",
    "\n",
    "    {\"item\": [\"04\", \"015\", \"950100\", \"1\"], \"probability\": 0.514, \"level\": \"blockgroup\"}\n",
    "    {\"item\": [\"04\", \"015\", \"950100\", \"3\"], \"probability\": 0.486, \"level\": \"blockgroup\"}\n",
    "\n",
    "Because zipcodes cover quite large areas, it doesn't know which blockgroup the person's home is in, as the zip code spans more than one blockgroup. It therefore gives the probability of being in the two.\n",
    "\n",
    "The US census module doesn't know about religion, so doesn't have a conditional probability distribution about it, so no religion feature is created. The features that are created are:\n",
    "\n",
    " - bg (block group)\n",
    " - gender\n",
    " - age"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Metadata *[action: metadata]*\n",
    "\n",
    "Some of the classes provide metadata about the results. Use the 'metadata' action to retrieve these. Pass a dictionary in 'data' with the name of the dataset, or leave empty to get all the metadata of all the classes.\n",
    "\n",
    "In this example we display the citation information for the 'babynames' dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The ONS provide statistics on the distribution of the names of baby's in the UK: <a href=\"http://www.ons.gov.uk/ons/about-ons/business-transparency/freedom-of-information/what-can-i-request/published-ad-hoc-data/pop/august-2014/baby-names-1996-2013.xls\">1996-2013</a> and <a href=\"http://www.ons.gov.uk/ons/rel/vsob1/baby-names--england-and-wales/1904-1994/top-100-baby-names-historical-data.xls\">1904-1994</a>.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "data = {'dataset':'babynames'}\n",
    "payload = {\"version\":1, 'data': data, 'apikey': 'YOUR_API_KEY_HERE'}\n",
    "r = requests.post(apiurl+'/metadata',json=payload)\n",
    "for item in json.loads(r.content):\n",
    "    if 'citation' in item:\n",
    "        print(item['citation'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example we get all citations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The <a href=\"facebook.com\">facebook</a> graph API\n",
      "The <a href=\"http://www.census.gov/developers/\">US census bureau</a>. In particular the American Community Survey <a href=\"http://www.census.gov/data/developers/data-sets/acs-survey-5-year-data.html\">5 year data</a>.\n",
      "The <a href=\"http://files.grouplens.org/datasets/movielens\">movielens</a> database\n",
      "The ONS provide statistics on the distribution of the names of baby's in the UK: <a href=\"http://www.ons.gov.uk/ons/about-ons/business-transparency/freedom-of-information/what-can-i-request/published-ad-hoc-data/pop/august-2014/baby-names-1996-2013.xls\">1996-2013</a> and <a href=\"http://www.ons.gov.uk/ons/rel/vsob1/baby-names--england-and-wales/1904-1994/top-100-baby-names-historical-data.xls\">1904-1994</a>.\n",
      "The <a href=\"https://geoportal.statistics.gov.uk\">UK office of national statistics</a> (see <a href=\"http://www.ons.gov.uk/ons/guide-method/geography/products/census/lookup/other/index.html\">details</a> and <a href=\"https://geoportal.statistics.gov.uk/geoportal/catalog/search/resource/details.page?uuid={A33B0569-97E2-4F44-836C-B656A6D082B6} \">information</a>) and the US zipcode data was from <a href=\"http://mcdc.missouri.edu\">Missouri's Census Data Center.\n",
      "The <a href=\"bandsintown.com\">bandsintown.com</a> API\n",
      "The <a href=\"http://www.ons.gov.uk/ons/guide-method/census/2011/census-data/ons-data-explorer--beta-/index.html\">UK office of national statistics</a>\n",
      "The <a href=\"freegeoip.net\">freegeoip.net</a> API\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "data = {}#no dataset specified (makes it output all metadata)\n",
    "payload = {\"version\":1, 'data': data, 'apikey': 'YOUR_API_KEY_HERE'}\n",
    "r = requests.post(apiurl+'/metadata',json=payload)\n",
    "for item in json.loads(r.content):\n",
    "    if 'citation' in item:\n",
    "        print(item['citation'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Typical API usage\n",
    "\n",
    "The scikic front end may use the API in the following way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What's your favourite band or artist? (be honest!)eels\n",
      "Have you seen Insider, The (1999)? (yes or no)no\n",
      "Have you seen Mad Max Beyond Thunderdome (1985)? (yes or no)no\n",
      "\n",
      "Insights\n",
      "\n",
      "Note: I can't see your facebook likes.\n",
      "OSM Answer system online\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "#We start with no questions asked, none unprocessed, and nothing in the facts dictionary.\n",
    "questions_asked = []\n",
    "unprocessed_questions = []\n",
    "facts = {}\n",
    "\n",
    "for loop in range(3): #we'll ask three questions\n",
    "    \n",
    "    #1. get the question (populate the data dictionary & send it off)\n",
    "    data = {'unprocessed_questions':unprocessed_questions,'questions_asked':questions_asked,'facts':facts}\n",
    "    payload = {\"version\":1, 'data': data, 'apikey': 'YOUR_API_KEY_HERE'}\n",
    "    r = requests.post(apiurl+'/question',json=payload) #>>>\n",
    "    question_query_result = json.loads(r.content)\n",
    "    \n",
    "    #if processing was done then more items will be available for 'facts':\n",
    "    facts = question_query_result['facts']\n",
    "    question = question_query_result['question']\n",
    "    #ask the user this question    \n",
    "    userinput = raw_input(question_query_result['question_string']['question'])\n",
    "    question['answer'] = userinput #add their answer\n",
    "\n",
    "    #add this to the list of questions we've asked, and unprocessed questions\n",
    "    questions_asked.append(question)\n",
    "    unprocessed_questions.append(question)\n",
    "    \n",
    "#3. Once enough questions are asked we can do inference.\n",
    "#   Populate the data dictionary with questions asked,\n",
    "#   unprocessed questions and facts (as for the question query in step 1)\n",
    "data = {'questions_asked':questions_asked,'unprocessed_questions':unprocessed_questions,'facts':facts}\n",
    "payload = {\"version\":1, 'data': data, 'apikey': 'YOUR_API_KEY_HERE'}\n",
    "r = requests.post(apiurl+'/inference',json=payload)\n",
    "inference_results = json.loads(r.content)\n",
    "\n",
    "#It generates insights from these questions, which are displayed below.\n",
    "print \"\\nInsights\\n\"\n",
    "for insight in inference_results['insights']:\n",
    "    print insight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Scikic System\n",
    "\n",
    "## Instances\n",
    "\n",
    "I'm not currently working on the front end, as that's being taken over by Vasily. The latest API server is running on the development server: dev.scikic.org.\n",
    "\n",
    "An autoscaling group, launch configuration and load balancer are used to provide the backend processing on the production machines. This will include SSL. The launch configuration uses the 'Production BackEnd Image' instance to produce the AMI.\n",
    "\n",
    "### Recipe for self to update backend\n",
    "\n",
    "1. On the dev system run nosetest.\n",
    "2. Push the changes to git\n",
    "3. On the production backend image server, pull the changes.\n",
    "4. Right click on server, create image.\n",
    "5. image name: production-backend-120116.\n",
    "\n",
    "6. Click on launch configurations\n",
    "7. Create Launch Cofiguration\n",
    "8. Select the new image name you just created\n",
    "9. Select t2.medium\n",
    "10. Name: production-backend-120116-lc\n",
    "11. Next> Delete both on termination.\n",
    "12. Select existing security group (production backend)\n",
    "13. Launch\n",
    "\n",
    "14. Autoscaling group. Select the production-backend-4-asg and click edit\n",
    "15. Select the new launch configuration\n",
    "\n",
    "\n",
    "## DNS\n",
    "\n",
    "DNS is provided by namecheap (see Neil for username/password). I've set the nameservers for scikic.org to point to the AWS Route53 DNS system.\n",
    "\n",
    "## How to configure in the future\n",
    "\n",
    "- cloudinit\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Stress Testing - Out of Date\n",
    "\n",
    "To test how well the system copes under load, this script calls the API multiple times:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching 240 processes over the next 120 seconds\n",
      "(launch)\n",
      "(launch)\n",
      "(launch)\n",
      "(launch)\n",
      "(launch)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-b02dbe0adf2e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[0mt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mThread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmyfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m         \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m120.\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mit\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import time\n",
    "from threading import Thread\n",
    "\n",
    "def myfunc(i):\n",
    "    print \"(launch)\"\n",
    "    questions_asked = [{'dataset':'postal','dataitem':'postcode','detail':'','answer':'s63af'}]\n",
    "    unprocessed_questions = [{'dataset':'postal','dataitem':'postcode','detail':'','answer':'s63af'}]\n",
    "    facts = {}\n",
    "        \n",
    "    data = {'questions_asked':questions_asked,'unprocessed_questions':unprocessed_questions,'facts':facts}\n",
    "    payload = {\"version\":1, 'data': data, 'apikey': 'YOUR_API_KEY_HERE', 'action':'inference'}\n",
    "    url = \"http://api.scikic.org\"\n",
    "    r = requests.post(url,json=payload)\n",
    "    print r.content\n",
    "\n",
    "it = 240\n",
    "for itn in range(1,600):\n",
    "    print \"Launching %d processes over the next 120 seconds\" % it\n",
    "    for i in range(it):\n",
    "        t = Thread(target=myfunc, args=(i,))\n",
    "        t.start()\n",
    "        time.sleep(120./it)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Questions\n",
    "\n",
    "- What happens if a class doesn't return a None question (but instead actually has a question)? But it also needs processing?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
