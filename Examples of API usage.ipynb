{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scikic API v9.0\n",
    "\n",
    "## Change log\n",
    "### Changes since v8.0\n",
    "\n",
    " - Added audience estimation\n",
    " \n",
    "### Changes since v7.0\n",
    "\n",
    " - Added Green Simple insight\n",
    " \n",
    "### Changes since v6.0\n",
    "\n",
    " - Added configuration option to inference to speed up MCMC, defaults to fast and poor inference\n",
    " - Added smoothing to try to hide some of the poor quality of the inference in the age distribution.\n",
    " \n",
    "### Changes since v5.0\n",
    "\n",
    " - Added relationships to the API's inference output (back compatible!)\n",
    "\n",
    "### Changes since v4.0\n",
    "\n",
    " - The metadata command takes a list for dataset, rather than an individual dataset.\n",
    "        data = {'dataset':['ukcensus','uscensus']}\n",
    "\n",
    "### Changes since v2.0\n",
    "\n",
    " - psychometrics from Cambridge now have new structure (and return percentiles)\n",
    " - insights is now a DICTIONARY not a list. So you can access particular answers.\n",
    " - inference can contain a list of dataset classes that it should use. If this is not passed it will use all the classes it can.\n",
    " \n",
    "## Overview\n",
    "\n",
    "The scikic api is an inference tool which takes a set of question/answer items and then queries a series of local and remote databases to generate conditional probability distributions over various features. The api is highly modular, and some modules don't use this probabilistic framework, for example the music module simply contacts api.bandsintown.com to provide useful suggestions about local bands to go and see.\n",
    "\n",
    "The conditional probabilities are combined using a Bayesian network, using the pyMC module. Each module can provide pyMC 'features' which create functions to output the relevant probability distributions.\n",
    "\n",
    "## Question/Answer dictionaries\n",
    "\n",
    "The questions and answers are organised to be in 4 value tuples, containing:\n",
    "\n",
    "- dataset: lets the system know which class to instantiate etc, examples: postcode, census, movielens, ...etc\n",
    "- dataitem: used by classes to know which aspect of the dataset. For example in the movielens dataset, one could be interested in whether the user's seen a film or what rating they've given the film.\n",
    "- detail: often unused by the classes, could be, for example, the id of the film we want to know about.\n",
    "- answer: the user's answer.\n",
    "\n",
    "## Future Changes/TODO list\n",
    "\n",
    "Heads-up about a couple of changes:\n",
    "\n",
    " - ~~Vasily suggested I move the 'action' selection out and make it part of the URL, to make it more RESTful.~~ DONE.\n",
    " - ~~I'll be combining the API query that gets the question string, with the one that generates the 'raw' question tuple. Originally the API was designed when it was run on the same server as the code that was querying it, so extra API calls didn't cost much. As the calls will now be remote, they need combining.~~ DONE\n",
    " - ~~document the new API~~ DONE (below)\n",
    " - fix security issues (stop outputting debug info to client)\n",
    " - add useful error messages to API's output\n",
    " - incorporate Cambridge's code\n",
    " - write a new install script for the setup\n",
    " - install on front end\n",
    " - The version number will be used in future.\n",
    " - The API doesn't currently check the key. Obviously this will be implemented in future.\n",
    " - Get the IP->Location working again (API change?)\n",
    " - Check load balancer to backend API still working (this shouldn't be need altering)\n",
    " - Set up SSL for API load balancer (previously left connection between frontend and backend non-SSL as it's all in the AWS datacentre).\n",
    "\n",
    "## Available actions\n",
    "\n",
    "Here are some examples of the API in action. A **POST** request is used for the query, in case the data we're sending is too large to fit in a GET request. The POST body is json.\n",
    "\n",
    "### 1. question\n",
    "\n",
    "#### Parameters\n",
    "One passes to this call in data, a dictionary containing:\n",
    " - 'questions_asked'\n",
    " - 'unprocessed_questions'\n",
    " - 'facts'\n",
    " - 'target'\n",
    "\n",
    "The 'questions_asked' include all the questions we've asked, so we don't ask the same question again.\n",
    "The 'facts' dictionary contains information that we've found from earlier questions, etc. It allows caching of the calculations from earlier calls to the API.\n",
    "The 'unprocessed_questions' are a list of question/answers that we've asked before, that haven't had their results added to the 'facts' dictionary.\n",
    "The 'target' item is currently unused, but in the future will allow the choice of question to be selected to maximise the information about a particular feature.\n",
    "\n",
    "#### Returns\n",
    "\n",
    "This call returns a dictionary containing three things:\n",
    "\n",
    " - a 'facts' dictionary - this you can pass back in future so that the method doesn't have to recalculate or generate earlier results.\n",
    " - a 'question' dictionary, containing the dictionary describing the question, e.g. {dataset,dataitem,detail}.\n",
    " - a 'question_string' dictionary, containing:\n",
    " \n",
    "    - 'text' - the actual string of the question (e.g. \"Who's your favourite band or artist?\")\n",
    "    - 'type' - the type of question (it might just want a text reply, so this would equal 'text' or it might be a choice, and so would say 'select'\n",
    "    - 'options' - optional, and is included if the type is 'select'.\n",
    "\n",
    "\n",
    "#### Usage example\n",
    "\n",
    "1. One might initially call this method with all these fields being empty. The method will return an empty 'facts' dictionary and a question dictionary for the first question you want to ask. \n",
    "2. Once you have an answer from the user you would call the method again, this time with the question/answer tuple as both 'questions_asked' and 'unprocessed_questions'.\n",
    "3. The method will return a facts dictionary now, potentially with some results from the processing of the last answer, and another question for you to ask. \n",
    "4. When you call the method a third time (with the user's second answer), you'll pass all the question/answer tuples that you've asked so far in 'questions_asked' and the last question/answer tuple in 'unprocessed_questions'. You'll also pass the new facts dictionary, that now has some content in it.\n",
    "5. This process continues, with the facts dictionary growing each time, the 'questions_asked' growing too, and each time you just have one item in 'unprocessed_questions'.\n",
    "\n",
    "To summarise:\n",
    "\n",
    "Generating a question requires a dictionary of 'questions_asked', 'facts' and 'target'. The 'questions_asked' is a list of dictionaries of previous questions, that you want to avoid asking again.\n",
    "The 'unprocessed_questions' are questions that you've asked already and that haven't been incorporated into the 'facts' dictionary.\n",
    "\n",
    "Below is an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#The url of the API currently (in the long run we should give the backend a domain, maybe api.scikic.org?)\n",
    "apiurl = 'http://api.scikic.org' #production api<<<\n",
    "#apiurl = 'http://dev.scikic.org' #development api\n",
    "#apiurl = 'http://54.194.2.173' #backend image server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"facts\": {\"guess_loc\": {}, \"where\": {}}, \"question_string\": {\"type\": \"select\", \"question\": \"Which country are you in now?\", \"options\": [\"United States\", \"United Kingdom\", \"Germany\", \"Canada\", \"France\", \"Other\"]}, \"question\": {\"detail\": \"{}\", \"dataitem\": \"country\", \"dataset\": \"geoloc\"}}'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "#We provide data about previous questions etc:\n",
    "#data consists of a dictionary of (all of these are optional):\n",
    "\n",
    "#'questions_asked': An array of previous questions and answers we've asked, consists of a list of dictionaries.\n",
    "questions_asked = [{'dataset':'postal','dataitem':'postcode','detail':''},{\"detail\": \"\", \"dataitem\": \"favourite_artist\", \"dataset\": \"music\"}]\n",
    "\n",
    "#none of the questions that have been asked before have been processed.\n",
    "unprocessed_questions = questions_asked\n",
    "\n",
    "#'facts': If you've run the inference query and stored a copy of the facts dictionary you can pass it back.\n",
    "#this makes it quicker. In this case we've not yet had any questions processed.\n",
    "facts = {}\n",
    "\n",
    "#'target': What feature we want to know more about (example: 'age', 'gender', 'location') NOT YET IMPLEMENTED\n",
    "#not used. All these items are optional, so we just don't include it.\n",
    "\n",
    "#Build the dictionary:\n",
    "data = {'unprocessed_questions':unprocessed_questions,'questions_asked':questions_asked,'facts':facts}\n",
    "\n",
    "#put it into the payload of the request. This also includes the version and the api key we're using\n",
    "payload = {\"version\":1, 'data': data, 'apikey': 'YOUR_API_KEY_HERE'}\n",
    "r = requests.post(apiurl+'/question',json=payload)\n",
    "r.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here it has output:\n",
    "\n",
    " - the facts dictionary: Empty\n",
    " - the question it suggests we ask, which in an earlier case is from the movielens dataset class, and is asking if they've seen movie number 2541.\n",
    " - the question string, with the type of question and options (if relevant). For example the dictionary `{'dataset':'postal','dataitem':'postcode','detail':''}` gets converted to the question \"`What's your postcode?`\" with type \"`text`\" (i.e. the user can type anything)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. inference\n",
    "\n",
    "#### Parameters\n",
    "\n",
    "The data dictionary should contain three things, similar to the \"action: question\" above,\n",
    "\n",
    "- questions_asked - list of question tuples that we've asked (with their answers)\n",
    "- unprocessed_questions - list of question tuples that we've asked (with their answers), that have not yet been added to the facts dictionary.\n",
    "- facts - the current 'facts' dictionary (possibly provided by earlier calls using action:question)\n",
    "\n",
    "**NEW** It optionally can contain a list of dataset classes that it should use. If this is not passed it will use all the classes it can.\n",
    "\n",
    "- datasets - list of datasets.\n",
    "\n",
    "#### Output\n",
    "\n",
    "Returns a dictionary of:\n",
    "\n",
    "##### features\n",
    "This is a dictionary of things that have probabilities associated, for example one of its items is 'household' with the following fields:\n",
    " \n",
    "     {\"distribution\": [0.029, 0.058, 0.23, 0.034, 0.070, 0.026, 0.055, 0.036, 0.14, 0.024, 0.023, 0.035, 0.24], \"quartiles\": {\"upper\": 11, \"lower\": 2, \"mean\": 6.46}}\n",
    " \n",
    " where the distribution is how likely the person is to be in each of the categories of a household (these categories can be found in the metadata from the module, or elsewhere). The quartiles don't mean much here as this is properly categorical data. This makes more sense in data such as age.\n",
    " \n",
    "##### facts\n",
    "As mentioned previously is a set of truths about the user generated from processing their answers.\n",
    "\n",
    "##### Insights\n",
    "** CHANGE **\n",
    "This is now a dictionary of strings, generated by each module, here is an example:\n",
    " \n",
    "     {u'ukcensus_traveltowork': u'People in your area are 3 times more likely to go to work on foot than the national average.', u'ukcensus_household': u\"You don't have children living at home\", u'ukcensus_ages': u'You are aged between 21 and 34.', u'ukcensus_religion': u' I think you are Christian or of no religion.', u'ukcensus_popage': u'Half the people in your neighbourhood are younger than 25 years old.', u'ukcensus_popdensity': u'Your neighbourhood has a population density of 1930 people per square kilometre, 4.7 times the average for England.'}\n",
    " \n",
    "Note that each string is identified by a label in the dictionary, allowing easier access to the strings by the client.\n",
    "\n",
    "*Note regarding the distribution above: If some probabilities are zero towards the end of a list then the list will be truncated. For example if inference is certain the user is a male, then the output list will be {\"factor_gender\":[1.0]}. If they are definitely female it will be {\"factor_gender\":[0.0, 1.0]}*\n",
    "\n",
    "##### Relationships\n",
    "** CHANGE **\n",
    "Allows the graphical model to be displayed on the web conversational interface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example call using action:inference\n",
    "\n",
    "Below we set up the data dictionary with one question asked (and answered) regarding postcode. It also contains the same question/answer dictionary as an unprocessed question. The facts dictionary is empty.\n",
    "\n",
    "The output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** THE UK ONS HAS WITHDRAWN API ACCESS THE FOLLOW EXAMPLE NO LONGER WORKS **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from timeit import default_timer as time\n",
    "\n",
    "postcode = 's63af'\n",
    "questions_asked = []\n",
    "\n",
    "#their address:\n",
    "#questions_asked.append({'dataset':'postal','dataitem':'postcode','detail':'','answer':'kuhdrfliudrh'})\n",
    "questions_asked.append({'dataset':'postal','dataitem':'postcode','detail':'','answer':'s63af'})\n",
    "#questions_asked.append({'dataset':'geoloc','dataitem':'country','detail':'','answer':'United Kingdom'})\n",
    "\n",
    "#Additional info from facebook:\n",
    "#fb_data = json.dumps({'reply':json.dumps({'first_name':'Alan'})})\n",
    "#questions_asked.append({'dataset':'facebook','dataitem':'','detail':'','answer':fb_data})\n",
    "\n",
    "datasets = ['demographic','postal','ukcensus','uscensus','babynames']\n",
    "#we assume that we've not processed any of these questions yet. \n",
    "#You'll only not do this if you've asked some of these already,\n",
    "#and populated the 'facts' dictionary.\n",
    "unprocessed_questions = questions_asked[:]\n",
    "facts = {}\n",
    "configuration = {'mcmc_iterations':100} #50,000 probably the most you'd want here. defaults to 1,000\n",
    "data = {'questions_asked':questions_asked,\n",
    "        'unprocessed_questions':unprocessed_questions,\n",
    "        'facts':facts,\n",
    "        'config':configuration}\n",
    "payload = {\"version\":1, 'data': data, 'apikey': 'YOUR_API_KEY_HERE'}\n",
    "\n",
    "t0 = time()\n",
    "r = requests.post(apiurl+'/inference',json=payload)\n",
    "t1 = time()\n",
    "print \"TOTAL TIME %0.0fms\" % (1000*(t1-t0))\n",
    "result = json.loads(r.content)\n",
    "print \"==Facts==\"\n",
    "print result['facts']\n",
    "print \"\"\n",
    "print \"==Features==\"\n",
    "for feat in result['features']:\n",
    "    print feat\n",
    "    print result['features'][feat]\n",
    "print \"\"\n",
    "print \"==Text insights==\"\n",
    "print result['insights']\n",
    "print \"==Relationships==\"\n",
    "print result['relationships']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the example above one can see that after just processing the answer about postcode, quite a bit of new info has been generated.\n",
    "\n",
    "First the facts dictionary. This is often dataset specific stuff, although I've tried to make things compatable between datasets.\n",
    "\n",
    " - 'guess_loc': {} - info on whether it guessed the location of the user from IP address (I think)\n",
    " - 'where': - the dictionary describing the location of the user's home. This is quite tricky, as different sources of data have different resolutions, etc about this.\n",
    "     - {'ukcensus': [{'item': 'E00172420', 'probability': 1.0, 'level': 'oa'}] - In terms of the UK census, with have a list of output areas for this person's home. This list only has one item in it. Each item in the list has a probability associated, in this case the probability is 1.0: We are certain the person is in that output area.\n",
    "     - 'country': [{'item': 'gb', 'probability': 1.0}] - Which country they're in. A list of countries with associated probabilities.\n",
    "     - 'city': [{'item': ['Sheffield', 'uk'], 'probability': 1.0}]} - which city their in (with probabilites).\n",
    "     - 'where_history': {u'error': u'no_fb_likes'}} - if we have access to facebook likes, it tries to generate a history of where the person's lived. But the error item means that it's not managed to get hold of the likes to do this.\n",
    "\n",
    "Next the features dictionary. Different datasets provide different conditional probability distributions. Each distribution has at least two features associated. If they are not in the list already the module adds them, thus one has a list of features at the end.\n",
    "\n",
    "The value of these features is then estimated using MCMC with the pyMC module.\n",
    "\n",
    "The example above has five features: religion, household, (census) output area, gender and age. They're all catagorical (for now) although that's purely due to the type of data that we have about them.\n",
    "\n",
    "Looking just at the household feature: [0.022222222222222223, 0.05688888888888889, 0.23466666666666666, 0.03244444444444444, 0.08266666666666667, 0.028, 0.059111111111111114, 0.03955555555555555, 0.14222222222222222, 0.024, 0.024444444444444446, 0.028444444444444446, 0.22533333333333333]\n",
    "\n",
    "Each value refers to the probability of being of a given type of household (the labels associated are available via the metadata of the module).\n",
    "\n",
    "Finally there are insights, these are simple strings of facts about the person.\n",
    " \n",
    "Here's another example, with an american zip code:\n",
    "\n",
    "<a id=\"usexample\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOTAL TIME 119ms\n",
      "{\"facts\": {\"age\": 33}, \"relationships\": [], \"feature_descriptions\": {\"factor_gender\": {\"desc\": \"Your gender\"}, \"factor_age\": {\"desc\": \"Your age\"}}, \"features\": {\"factor_age\": {\"distribution\": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03162277660168379, 0.03162277660168379, 0.03162277660168379, 0.03162277660168379, 0.03162277660168379, 0.03162277660168379, 0.03162277660168379, 0.03162277660168379, 0.03162277660168379, 0.03162277660168379, 0.03162277660168379, 0.03162277660168379, 0.03162277660168379, 0.03162277660168379, 0.03162277660168379, 0.03162277660168379], \"quartiles\": {\"upper\": 33, \"lower\": 33, \"mean\": 33.0}}}, \"insights\": {\"debug_total_inference_time\": \"53.290844ms\"}}\n",
      "==Facts==\n",
      "{u'age': 33}\n",
      "\n",
      "==Features==\n",
      "factor_age\n",
      "{u'distribution': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03162277660168379, 0.03162277660168379, 0.03162277660168379, 0.03162277660168379, 0.03162277660168379, 0.03162277660168379, 0.03162277660168379, 0.03162277660168379, 0.03162277660168379, 0.03162277660168379, 0.03162277660168379, 0.03162277660168379, 0.03162277660168379, 0.03162277660168379, 0.03162277660168379, 0.03162277660168379], u'quartiles': {u'upper': 33, u'lower': 33, u'mean': 33.0}}\n",
      "\n",
      "==Text insights==\n",
      "{u'debug_total_inference_time': u'53.290844ms'}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "from timeit import default_timer as time\n",
    "\n",
    "questions_asked = []\n",
    "\n",
    "#their address:\n",
    "#questions_asked.append({'dataset':'postal','dataitem':'zipcode','detail':'','answer':'86020'})\n",
    "questions_asked.append({'dataset':'postal','dataitem':'zipcode','detail':'','answer':'90210'})\n",
    "questions_asked.append({'dataset':'demographic','dataitem':'age','detail':'','answer':'33'})\n",
    "\n",
    "datasets = ['demographic','postal','ukcensus','uscensus']\n",
    "#we assume that we've not processed any of these questions yet. \n",
    "#You'll only not do this if you've asked some of these already,\n",
    "#and populated the 'facts' dictionary.\n",
    "unprocessed_questions = questions_asked[:]\n",
    "facts = {}\n",
    "data = {'questions_asked':questions_asked,'unprocessed_questions':unprocessed_questions,'facts':facts,'datasets':datasets}\n",
    "payload = {\"version\":1, 'data': data, 'apikey': 'YOUR_API_KEY_HERE'}\n",
    "\n",
    "t0 = time()\n",
    "r = requests.post(apiurl+'/inference',json=payload)\n",
    "t1 = time()\n",
    "print \"TOTAL TIME %0.0fms\" % (1000*(t1-t0))\n",
    "\n",
    "print r.content\n",
    "\n",
    "result = json.loads(r.content)\n",
    "print \"==Facts==\"\n",
    "print result['facts']\n",
    "print \"\"\n",
    "print \"==Features==\"\n",
    "for feat in result['features']:\n",
    "    print feat\n",
    "    print result['features'][feat]\n",
    "print \"\"\n",
    "print \"==Text insights==\"\n",
    "print result['insights']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see this is similar, except the 'where' item in the dictionary has a 'uscensus' item within it. This contains two items:\n",
    "\n",
    "    {\"item\": [\"04\", \"015\", \"950100\", \"1\"], \"probability\": 0.514, \"level\": \"blockgroup\"}\n",
    "    {\"item\": [\"04\", \"015\", \"950100\", \"3\"], \"probability\": 0.486, \"level\": \"blockgroup\"}\n",
    "\n",
    "Because zipcodes cover quite large areas, it doesn't know which blockgroup the person's home is in, as the zip code spans more than one blockgroup. It therefore gives the probability of being in the two.\n",
    "\n",
    "The US census module doesn't know about religion, so doesn't have a conditional probability distribution about it, so no religion feature is created. The features that are created are:\n",
    "\n",
    " - bg (block group)\n",
    " - gender\n",
    " - age"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Metadata *[action: metadata]*\n",
    "\n",
    "Some of the classes provide metadata about the results. Use the 'metadata' action to retrieve these. Pass a dictionary in 'data' with the names of the datasets, or leave empty to get all the metadata of all the classes.\n",
    "\n",
    "Change in v5: The metadata takes a list for dataset, rather than an individual dataset.\n",
    "        \n",
    "    data = {'dataset':['ukcensus','uscensus']}\n",
    "\n",
    "In this example we display the citation information for the 'babynames' dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The <a href=\"http://www.census.gov/developers/\">US census bureau</a>. In particular the American Community Survey <a href=\"http://www.census.gov/data/developers/data-sets/acs-survey-5-year-data.html\">5 year data</a>.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "data = {'dataset':['uscensus']}\n",
    "payload = {\"version\":1, 'data': data, 'apikey': 'YOUR_API_KEY_HERE'}\n",
    "r = requests.post(apiurl+'/metadata',json=payload)\n",
    "for item in json.loads(r.content):\n",
    "    if 'citation' in item:\n",
    "        print(item['citation'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example we get the list of languages supported by the uk census"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'English', u'Spanish', u'French', u'French Creole', u'Italian', u'Portuguese or Portuguese Creole', u'German', u'Yiddish', u'a Scandinavian language', u'Greek', u'Russian', u'Polish', u'Serbo-Croatian', u'Armenian', u'Persian', u'Gujarati', u'Hindi', u'Urdu', u'Chinese', u'Japanese', u'Korean', u'Mon-Khmer, Cambodian', u'Hmong', u'Thai', u'Laotian', u'Vietnamese', u'Tagalog', u'Navajo', u'Hungarian', u'Arabic', u'Hebrew', u'an African language']\n",
      "[u'Afrikaans', u'Akan', u'Amharic', u'Igbo', u'Krio', u'Lingala', u'Luganda', u'Shona', u'Somali', u'Swahili', u'Tigrinya', u'Yoruba', u'Arabic', u'Caribbean Creole', u'Cantonese Chinese', u'Japanese', u'Korean', u'Malay', u'Mandarin Chinese', u'Tagalog/Filipino', u'Thai', u'Vietnamese', u'English', u'French', u'Bulgarian', u'Czech', u'Danish', u'Dutch', u'Estonian', u'Finnish', u'German', u'Greek', u'Hungarian', u'Italian', u'Latvian', u'Lithuanian', u'Maltese', u'Polish', u'Romanian', u'Slovak', u'Slovenian', u'Swedish', u'Albanian', u'Serbian, Croatian or Bosnian', u'Ukrainian', u'Yiddish', u'Cornish', u'Irish Gaelic', u'Gaelic', u'Scottish Gaelic', u'Scots', u'Portuguese', u'Russian', u'Sign Language', u'Bengali', u'Gujarati', u'Hindi', u'Malayalam', u'Marathi', u'Nepalese', u'Pakistani Pahari', u'Punjabi', u'Sinhala', u'Tamil', u'Telugu', u'Urdu', u'Spanish', u'Turkish', u'Welsh', u'Hebrew', u'Kurdish', u'Pashto', u'Farsi']\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "data = {'dataset':['ukcensus','uscensus']}\n",
    "payload = {\"version\":1, 'data': data, 'apikey': 'YOUR_API_KEY_HERE'}\n",
    "r = requests.post(apiurl+'/metadata',json=payload)\n",
    "for item in json.loads(r.content):\n",
    "    if 'languages_text' in item:\n",
    "        print(item['languages_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example we get a list of all the datasets available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "babynames\n",
      "music\n",
      "uscensus\n",
      "geoloc\n",
      "facebook\n",
      "postal\n",
      "ukcensus\n",
      "movielens\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "data = {}#no dataset specified (makes it output all metadata)\n",
    "payload = {\"version\":1, 'data': data, 'apikey': 'YOUR_API_KEY_HERE'}\n",
    "r = requests.post(apiurl+'/metadata',json=payload)\n",
    "for item in json.loads(r.content):\n",
    "    if 'citation' in item:\n",
    "        print(item['dataset'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example we get all citations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The <a href=\"http://files.grouplens.org/datasets/movielens\">movielens</a> database\n",
      "The <a href=\"http://www.ons.gov.uk/ons/guide-method/census/2011/census-data/ons-data-explorer--beta-/index.html\">UK office of national statistics</a>\n",
      "The ONS provide statistics on the distribution of the names of baby's in the UK: <a href=\"http://www.ons.gov.uk/ons/about-ons/business-transparency/freedom-of-information/what-can-i-request/published-ad-hoc-data/pop/august-2014/baby-names-1996-2013.xls\">1996-2013</a> and <a href=\"http://www.ons.gov.uk/ons/rel/vsob1/baby-names--england-and-wales/1904-1994/top-100-baby-names-historical-data.xls\">1904-1994</a>.\n",
      "The <a href=\"facebook.com\">facebook</a> graph API\n",
      "The <a href=\"http://www.census.gov/developers/\">US census bureau</a>. In particular the American Community Survey <a href=\"http://www.census.gov/data/developers/data-sets/acs-survey-5-year-data.html\">5 year data</a>.\n",
      "The <a href=\"freegeoip.net\">freegeoip.net</a> API\n",
      "The <a href=\"https://geoportal.statistics.gov.uk\">UK office of national statistics</a> (see <a href=\"http://www.ons.gov.uk/ons/guide-method/geography/products/census/lookup/other/index.html\">details</a> and <a href=\"https://geoportal.statistics.gov.uk/geoportal/catalog/search/resource/details.page?uuid={A33B0569-97E2-4F44-836C-B656A6D082B6} \">information</a>) and the US zipcode data was from <a href=\"http://mcdc.missouri.edu\">Missouri's Census Data Center.\n",
      "The <a href=\"bandsintown.com\">bandsintown.com</a> API\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "data = {}#no dataset specified (makes it output all metadata)\n",
    "payload = {\"version\":1, 'data': data, 'apikey': 'YOUR_API_KEY_HERE'}\n",
    "r = requests.post(apiurl+'/metadata',json=payload)\n",
    "for item in json.loads(r.content):\n",
    "    if 'citation' in item:\n",
    "        print(item['citation'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example we get the household types:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====ukcensus===\n",
      "households text\n",
      "[u'Cohabiting couple (children have left home)', u'Cohabiting couple with children', u'Cohabiting couple, without children', u'Single person (children have left home)', u'Lone parent', u'Married couple (children have left home)', u'Married couple with children', u'Married couple, without children', u'Single person', u'Other households, with children', u'Retired couple', u'Retired single person', u'Students and retired']\n",
      "household census labels\n",
      "[u'One family only: Cohabiting couple: All children non-dependent', u'One family only: Cohabiting couple: Dependent children', u'One family only: Cohabiting couple: No children', u'One family only: Lone parent: All children non-dependent', u'One family only: Lone parent: Dependent children', u'One family only: Married or same-sex civil partnership couple: All children non-dependent', u'One family only: Married or same-sex civil partnership couple: Dependent children', u'One family only: Married or same-sex civil partnership couple: No children', u'One person household: Other', u'Other household types: With dependent children', u'One family only: All aged 65 and over', u'One person household: Aged 65 and over', u'Other household types: Other (including all full-time students and all aged 65 and over)']\n",
      "====uscensus===\n",
      "households text\n",
      "[u'All family household', u'Married couple', u'Other family household', u'Single dad', u'Single mum', u'Non-family household', u'Single person', u'Shared property']\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "data = {'dataset':['ukcensus','uscensus']}\n",
    "payload = {\"version\":1, 'data': data, 'apikey': 'YOUR_API_KEY_HERE'}\n",
    "r = requests.post(apiurl+'/metadata',json=payload)\n",
    "for item in json.loads(r.content):\n",
    "    print '===='+item['dataset']+'==='\n",
    "    if 'households_text' in item:\n",
    "        print \"households text\"\n",
    "        print item['households_text']\n",
    "    if 'households_census_labels' in item:\n",
    "        print \"household census labels\"\n",
    "        print item['households_census_labels']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this code we're just getting all the metadata:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "-----------------------------------\n",
      "uscensus\n",
      " \n",
      "households_text\n",
      "[u'All family household', u'Married couple', u'Other family household', u'Single dad', u'Single mum', u'Non-family household', u'Single person', u'Shared property']\n",
      " \n",
      "birthplace_text\n",
      "[u'Born in state of residence', u'Born in other state in Northeast US', u'Born in other state in Midwest US', u'Born in other state in South US', u'Born in other state in West US', u'Born outside the US']\n",
      " \n",
      "birthplace_codes\n",
      "[u'B05002_003E', u'B05002_005E', u'B05002_006E', u'B05002_007E', u'B05002_008E', u'B05002_013E']\n",
      " \n",
      "households_codes\n",
      "[u'B11001_002E', u'B11001_003E', u'B11001_004E', u'B11001_005E', u'B11001_006E', u'B11001_007E', u'B11001_008E', u'B11001_009E']\n",
      " \n",
      "_states\n",
      "[u'Mo29', u'Al01', u'Ak02', u'Az04', u'Ar05', u'Ca06', u'Co08', u'Ct09', u'De10', u'Dc11', u'Fl12', u'Ga13', u'Hi15', u'Id16', u'Il17', u'In18', u'Ia19', u'Ks20', u'Ky21', u'La22', u'Me23', u'Md24', u'Ma25', u'Mi26', u'Mn27', u'Ms28', u'Mt30', u'Ne31', u'Nv32', u'Nh33', u'Nj34', u'Nm35', u'Ny36', u'Nc37', u'Nd38', u'Oh39', u'Ok40', u'Or41', u'Pa42', u'Ri44', u'Sc45', u'Sd46', u'Tn47', u'Tx48', u'Ut49', u'Vt50', u'Va51', u'Wa53', u'Wv54', u'Wi55', u'Wy56']\n",
      " \n",
      "citation\n",
      "The <a href=\"http://www.census.gov/developers/\">US census bureau</a>. In particular the American Community Survey <a href=\"http://www.census.gov/data/developers/data-sets/acs-survey-5-year-data.html\">5 year data</a>.\n",
      " \n",
      "dataset\n",
      "uscensus\n",
      " \n",
      "_age_range\n",
      "[5, 10, 15, 18, 20, 21, 22, 25, 30, 35, 40, 45, 50, 55, 60, 62, 65, 67, 70, 75, 80, 85, 101]\n",
      " \n",
      "language_codes\n",
      "[u'B16001_002E', u'B16001_003E', u'B16001_006E', u'B16001_009E', u'B16001_012E', u'B16001_015E', u'B16001_018E', u'B16001_021E', u'B16001_027E', u'B16001_030E', u'B16001_033E', u'B16001_036E', u'B16001_039E', u'B16001_045E', u'B16001_048E', u'B16001_051E', u'B16001_054E', u'B16001_057E', u'B16001_066E', u'B16001_069E', u'B16001_072E', u'B16001_075E', u'B16001_078E', u'B16001_081E', u'B16001_084E', u'B16001_087E', u'B16001_093E', u'B16001_099E', u'B16001_105E', u'B16001_108E', u'B16001_111E', u'B16001_114E']\n",
      " \n",
      "languages_text\n",
      "[u'English', u'Spanish', u'French', u'French Creole', u'Italian', u'Portuguese or Portuguese Creole', u'German', u'Yiddish', u'a Scandinavian language', u'Greek', u'Russian', u'Polish', u'Serbo-Croatian', u'Armenian', u'Persian', u'Gujarati', u'Hindi', u'Urdu', u'Chinese', u'Japanese', u'Korean', u'Mon-Khmer, Cambodian', u'Hmong', u'Thai', u'Laotian', u'Vietnamese', u'Tagalog', u'Navajo', u'Hungarian', u'Arabic', u'Hebrew', u'an African language']\n",
      "\n",
      "\n",
      "-----------------------------------\n",
      "ukcensus\n",
      " \n",
      "households_census_labels\n",
      "[u'One family only: Cohabiting couple: All children non-dependent', u'One family only: Cohabiting couple: Dependent children', u'One family only: Cohabiting couple: No children', u'One family only: Lone parent: All children non-dependent', u'One family only: Lone parent: Dependent children', u'One family only: Married or same-sex civil partnership couple: All children non-dependent', u'One family only: Married or same-sex civil partnership couple: Dependent children', u'One family only: Married or same-sex civil partnership couple: No children', u'One person household: Other', u'Other household types: With dependent children', u'One family only: All aged 65 and over', u'One person household: Aged 65 and over', u'Other household types: Other (including all full-time students and all aged 65 and over)']\n",
      " \n",
      "countryofbirth_labels\n",
      "[u'England', u'Ireland', u'Northern Ireland', u'Other countries', u'Scotland', u'United Kingdom not otherwise specified', u'Wales', u'Other EU: Accession countries April 2001 to March 2011', u'Other EU: Member countries in March 2001']\n",
      " \n",
      "transport_text\n",
      "[u'take a taxi to work', u'cycle to work', u'go to work on foot', u'not be in work', u'mainly work from home', u'use a motorcycle to get to work', u'take the bus to work', u'take the train to work', u'use an underground or tram to get to work', u'get a lift in a car to work', u'drive to work', u'use an unusual method of travel to get to work']\n",
      " \n",
      "citation\n",
      "The <a href=\"http://www.ons.gov.uk/ons/guide-method/census/2011/census-data/ons-data-explorer--beta-/index.html\">UK office of national statistics</a>\n",
      " \n",
      "dataset\n",
      "ukcensus\n",
      " \n",
      "languages\n",
      "[u'African Language: Afrikaans', u'African Language: Akan', u'African Language: Amharic', u'African Language: Igbo', u'African Language: Krio', u'African Language: Lingala', u'African Language: Luganda', u'African Language: Shona', u'African Language: Somali', u'African Language: Swahili/Kiswahili', u'African Language: Tigrinya', u'African Language: Yoruba', u'Arabic', u'Caribbean Creole: Caribbean Creole (English-based)', u'East Asian Language: Cantonese Chinese', u'East Asian Language: Japanese', u'East Asian Language: Korean', u'East Asian Language: Malay', u'East Asian Language: Mandarin Chinese', u'East Asian Language: Tagalog/Filipino', u'East Asian Language: Thai', u'East Asian Language: Vietnamese', u'English (English or Welsh if in Wales)', u'French', u'Other European Language (EU): Bulgarian', u'Other European Language (EU): Czech', u'Other European Language (EU): Danish', u'Other European Language (EU): Dutch', u'Other European Language (EU): Estonian', u'Other European Language (EU): Finnish', u'Other European Language (EU): German', u'Other European Language (EU): Greek', u'Other European Language (EU): Hungarian', u'Other European Language (EU): Italian', u'Other European Language (EU): Latvian', u'Other European Language (EU): Lithuanian', u'Other European Language (EU): Maltese', u'Other European Language (EU): Polish', u'Other European Language (EU): Romanian', u'Other European Language (EU): Slovak', u'Other European Language (EU): Slovenian', u'Other European Language (EU): Swedish', u'Other European Language (non EU): Albanian', u'Other European Language (non EU): Serbian/Croatian/Bosnian', u'Other European Language (non EU): Ukrainian', u'Other European Language (non-national): Yiddish', u'Other UK language: Cornish', u'Other UK language: Gaelic (Irish)', u'Other UK language: Gaelic (Not otherwise specified)', u'Other UK language: Gaelic (Scottish)', u'Other UK language: Scots', u'Portuguese', u'Russian', u'Sign Language: Any Sign Communication System', u'South Asian Language: Bengali (with Sylheti and Chatgaya)', u'South Asian Language: Gujarati', u'South Asian Language: Hindi', u'South Asian Language: Malayalam', u'South Asian Language: Marathi', u'South Asian Language: Nepalese', u'South Asian Language: Pakistani Pahari (with Mirpuri and Potwari)', u'South Asian Language: Panjabi', u'South Asian Language: Sinhala', u'South Asian Language: Tamil', u'South Asian Language: Telugu', u'South Asian Language: Urdu', u'Spanish', u'Turkish', u'Welsh/Cymraeg (in England only)', u'West/Central Asian Language: Hebrew', u'West/Central Asian Language: Kurdish', u'West/Central Asian Language: Pashto', u'West/Central Asian Language: Persian/Farsi']\n",
      " \n",
      "religions\n",
      "[u'Christian', u'Buddhist', u'Hindu', u'Jewish', u'Muslim', u'Sikh', u'Other religion', u'No religion']\n",
      " \n",
      "households_text\n",
      "[u'Cohabiting couple (children have left home)', u'Cohabiting couple with children', u'Cohabiting couple, without children', u'Single person (children have left home)', u'Lone parent', u'Married couple (children have left home)', u'Married couple with children', u'Married couple, without children', u'Single person', u'Other households, with children', u'Retired couple', u'Retired single person', u'Students and retired']\n",
      " \n",
      "religion_text\n",
      "[u'Christian', u'Buddhist', u'Hindu', u'Jewish', u'Muslim', u'Sikh', u\"religious (but I do't know which)\", u'of no religion']\n",
      " \n",
      "languages_text\n",
      "[u'Afrikaans', u'Akan', u'Amharic', u'Igbo', u'Krio', u'Lingala', u'Luganda', u'Shona', u'Somali', u'Swahili', u'Tigrinya', u'Yoruba', u'Arabic', u'Caribbean Creole', u'Cantonese Chinese', u'Japanese', u'Korean', u'Malay', u'Mandarin Chinese', u'Tagalog/Filipino', u'Thai', u'Vietnamese', u'English', u'French', u'Bulgarian', u'Czech', u'Danish', u'Dutch', u'Estonian', u'Finnish', u'German', u'Greek', u'Hungarian', u'Italian', u'Latvian', u'Lithuanian', u'Maltese', u'Polish', u'Romanian', u'Slovak', u'Slovenian', u'Swedish', u'Albanian', u'Serbian, Croatian or Bosnian', u'Ukrainian', u'Yiddish', u'Cornish', u'Irish Gaelic', u'Gaelic', u'Scottish Gaelic', u'Scots', u'Portuguese', u'Russian', u'Sign Language', u'Bengali', u'Gujarati', u'Hindi', u'Malayalam', u'Marathi', u'Nepalese', u'Pakistani Pahari', u'Punjabi', u'Sinhala', u'Tamil', u'Telugu', u'Urdu', u'Spanish', u'Turkish', u'Welsh', u'Hebrew', u'Kurdish', u'Pashto', u'Farsi']\n",
      " \n",
      "transport\n",
      "[u'Taxi', u'Bicycle', u'On foot', u'Not in employment', u'Work mainly at or from home', u'Motorcycle, scooter or moped', u'Bus, minibus or coach', u'Train', u'Underground, metro, light rail, tram', u'Passenger in a car or van', u'Driving a car or van', u'Other method of travel to work']\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "data = {'dataset':['ukcensus','uscensus']}\n",
    "payload = {\"version\":1, 'data': data, 'apikey': 'YOUR_API_KEY_HERE'}\n",
    "r = requests.post(apiurl+'/metadata',json=payload)\n",
    "for items in json.loads(r.content):\n",
    "    print \"\\n\\n-----------------------------------\"\n",
    "    print items['dataset']\n",
    "    for item in items:\n",
    "        print \" \"\n",
    "        print item\n",
    "        print items[item]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Psychometrics, provided by Cambridge\n",
    "\n",
    "This calls the psychometric analysis, provided by Cambridge.\n",
    "\n",
    "Pass in a string called 'userstatus'. The API returns a directionary of two dictionaries, one called \"percentiles\" and one called \"scores\".\n",
    "\n",
    "Each of these has five items 'ope', 'con', 'ext', 'agr', 'neu', which relate to the big5 personality features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"percentiles\": {\"ope\": 65.03, \"con\": 30.19, \"ext\": 49.19, \"agr\": 34.54, \"neu\": 24.2}, \"scores\": {\"ope\": 4.16, \"con\": 3.1, \"ext\": 3.72, \"agr\": 3.27, \"neu\": 2.21}}'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "userstatus = \"\"\" LOL! 'Take a deep breath\\n2. think of someone u like\\n3. press F10 5 times\\n4. send this to 5 coments\\n5. look at ur background\\n9 minutes ago ', 'Rotate your facebook friends omg =).  http://tinyurl.com/37wdxok', 'sarap!!!!!!!!!!!!!!!!!1', '???? Happy New Year In Advance!! ????? ?????????????????????????????????? ????????????????????????????????? ???????????????????????????? ????? ?????????????????????????????????? ?????????????????????????????????? ?????????????????????????????????? ??????????????????????????????????', 'Rotate your facebook friends omg =).  http://tinyurl.com/2ug8r4c', \"Apple finally gives away 100 Iphone 4's\\n http://apps.facebook.com/qbquiz-ghflb\", 'ano susuutin bukas ??????', 'Rotate your facebook friends omg =).  http://tinyurl.com/3xngp4s', 'Rotate your facebook friends omg =).  http://tinyurl.com/3354776', ':)?', 'Rotate your facebook friends omg =).  http://tinyurl.com/35ssbf9', 'AnOng sTanza Kana Roseangeli Salvador', 'Rotate your facebook friends omg =).  http://tinyurl.com/2vvpw55', 'Rotate your facebook friends omg =).  http://tinyurl.com/2wmv4au', 'Rotate your facebook friends omg =).  http://tinyurl.com/32f7z4z', '-', 'Pls like 4 ST PAUL!!!!!!!!!!?', 'HAPPY NEW YEAR!!!!!', 'EXAM NA BUKAS!!!!!!!!!? ?', 'POINTER TO REVIEW IN READING 4\\n-synonyms and antonyms\\n-words with silent letters\\n -simile and methaphor\\n-idiomatic expressions\\n-consonant digraphs\\ngaling kay ms nabong yan', 'Rotate your facebook friends omg =).  http://tinyurl.com/346gw6w', 'POINTER TO REVIEW IN READING 4-synonyms and antonyms-words with silent letters -simile and methaphor-idiomatic expressions-consonant digraphsgaling kay ms nabong yan', 'IS WATCHING SHOUTOUT', 'Rotate your facebook friends omg =).  http://tinyurl.com/37ne983', \"Apple finally gives away 100 Iphone 4's http://apps.facebook.com/rxytnkgo\", 'Rotate your facebook friends omg =).  http://tinyurl.com/35tplvh', 'Rotate your facebook friends omg =).  http://tinyurl.com/33oua5o', 'Rotate your facebook friends omg =).  http://tinyurl.com/38k6hmn', 'Rotate your facebook friends omg =).  http://tinyurl.com/2w2o7am', 'Rotate your facebook friends omg =).  http://tinyurl.com/3ajg5b3', 'Gwyneth Erin M. Nohay ano size nung cardcase?', \"------[]------ put this\\n--[][][][][]-- as your\\n------[]------ status if\\n------[]------ you're a christian\\n------[]------ and not ashamed\", 'te Shiela Mae Tolentino thankyou sa psp and chocolates:)', '??????????????????????????? ??????????????????????????? ???????????????????????????\\n\\n                             vs\\n\\n?????????????????????????? \\n?????????????????????????? \\n??????????????????????????', \"TEACHER: juan! may 5 ibon na nakaupo sa bakod, pag binaril mo ang isa..ilan ang matitira?\\n\\nJUAN: wala po ma'am.\\n\\nTEACHER: bobo ka ba?! bat mo nasabing wala?! sige nga?!\\n\\nJUAN: dahil isang putok mo lang ng baril aalis lahat ng ibon at walang matitira! bobo! ikaw ang maupo dito, at ako ang magtuturo!\\n\\n// matalino XD\", '???????????????????????\\n??????? ? If you love JESUS\\n????????????copy this in your wall ???????????????????????\\n?????????????????????\\n??????? ? ?', 'This year October has 5 Mondays, 5 Saturdays and 5 Sundays. This Happens once every 823years. This is called money bags. So copy this to your status and money will arrive within 4days. Based on Chinese Feng Shui. The one who does not copy, will be without money', \"I know 10 facts about you\\n\\n1. You are reading this.\\n2. You can't say M without touching your lips\\n3. You just tried it\\n... 4. You just smiled or laughed.\\n6. You are a boy/girl.\\n7. You didnt realize I skipped 5.\\n8. You are looking back at 4 and 6.\\n9. You are liking this.\\n10. You are reading me telling you to like this.\\n\\nLIKE THIS STATUS IF YOU DID THOSE THINGS\"]\"\"\"\n",
    "\n",
    "#Build the dictionary:\n",
    "data = {'userstatus':userstatus}\n",
    "\n",
    "#put it into the payload of the request. This also includes the version, the api key we're using and the action we want (in this case generate a question)\n",
    "payload = {\"version\":1, 'data': data, 'apikey': 'YOUR_API_KEY_HERE'}\n",
    "r = requests.post(apiurl+'/psych',json=payload)\n",
    "r.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Simple Insights\n",
    "\n",
    "This part, like the Cambridge psychometrics, doesn't use the scikic engine, but instead runs separately performing very simple inferences or responses.\n",
    "\n",
    "#### 5.1. Green Insights\n",
    "\n",
    "I don't know what format a json raw question set would be, so currenty it just accepts a list of lists of lists,\n",
    "the outer list is the 4 categories, the middle list is a list of questions (within each category). The inner list allows the user to give more than one answer to each question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All the scores given\n",
      "   - Category 0\n",
      "      20 100 100 0 0 0 100 \n",
      "   - Category 1\n",
      "      0 100 100 100 \n",
      "   - Category 2\n",
      "      40 40 40 \n",
      "   - Category 3\n",
      "      0 0 \n",
      "\n",
      "The category scores\n",
      "[44.0, 60.0, 40.0, 0.0]\n",
      "\n",
      "The overall score is 43.60\n",
      "\n",
      "The text insight(s)\n",
      "   - Occasionally green\n",
      "\n",
      "Debug Messages\n",
      "[[[u'heating', u'When at home during the winter, how warm do you keep your house?', [0]], [u'aircon', u'Similarly when at home during the summer, to what temperature do you set your digital air conditioner to?', [0]], [u'lights', u\"Do you instantly turn the lights off when you don't need them anymore e.g. when you switch over to another room?\", [0]], [u'tumble', u'How regularly, if at all, do you tumble dry your clothes?', [0]], [u'bike', u'Do you walk or bike short distances e.g. when you pop-over to the local shop or for running quick errands?', [0]], [u'recycle', u'How much do you recycle at home?', [0]], [u'bags', u'How often do you use the plastic bags provided at supermarkets and shops at checkout?', [0]], [u'energy', u'Thinking about your home energy (gas & electricity), are you on a green tariff?', [0]], [u'glazing', u'Do your windows have double-glazing', [0]], [u'insulation', u'How good would you consider your wall and floor insulation to be?', [0]], [u'systems', u'Which, if any, of these green energy systems do you have in your home?', [0, 1]], [u'energyrating', u\"Is a home appliance's energy rating an important consideration at time of purchase?\", [0]], [u'ecofriendly', u\"When out grocery shopping, is a product's eco-friendliness an important consideration at time of purchase?\", [0]], [u'packaging', u'Do you choose products with minimal/re-usable packaging whenever possible?', [0]], [u'car', u'When you last bought a car, if ever, how important was its carbon emissions in your decision making process?', [0]], [u'publictrasport', u'Is public transport a preferred means of transport for you?', [0]]]]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "#Build the dictionary:\n",
    "data = {'answers':[[[0],[3],[2],[1],[3],[4],[5]],[[1],[2],[3],[4,1,2,3]],[[2],[1],[0]],[[1],[2]]]}\n",
    "data = {'answers':[[[0],[0],[0],[0],[0],[0],[0]],[[0],[0],[0],[0,1]],[[0],[0],[0]],[[0],[0]]]}\n",
    "#data = {'answers_json':json_response_from_client}\n",
    "\n",
    "#put it into the payload of the request. This also includes the version, the api key we're using and the action we want (in this case generate a question)\n",
    "payload = {\"version\":1, 'data': data, 'apikey': 'YOUR_API_KEY_HERE'}\n",
    "r = requests.post(apiurl+'/simple/green',json=payload)\n",
    "response = json.loads(r.content)\n",
    "\n",
    "print(\"All the scores given\")\n",
    "for i,cat_scores in enumerate(response['values']['all_scores']):\n",
    "    print(\"   - Category %d\" % i)\n",
    "    print(\"     \"),\n",
    "    for score in cat_scores:\n",
    "        print(score),\n",
    "    print(\"\")\n",
    "\n",
    "print(\"\")\n",
    "print(\"The category scores\")\n",
    "print response['values']['category_scores']\n",
    "print(\"\")\n",
    "print(\"The overall score is %0.2f\" % response['values']['overall_score'])\n",
    "print(\"\")\n",
    "print(\"The text insight(s)\")\n",
    "for insight in response['insights']:\n",
    "    print(\"   - \" + insight)\n",
    "print(\"\")\n",
    "print(\"Debug Messages\")\n",
    "print(response['values']['debug'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[38.0, 36.0, 54.0, 50.0]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response['values']['category_scores']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1\n",
      "2 2\n"
     ]
    }
   ],
   "source": [
    "for i,j in zip([1,2,3],[1,2]):\n",
    "    print i,j"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Audience size prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE HTML PUBLIC \"-//W3C//DTD HTML 3.2 Final//EN\">\n",
      "<title>404 Not Found</title>\n",
      "<h1>Not Found</h1>\n",
      "<p>The requested URL was not found on the server.  If you entered the URL manually please check your spelling and try again.</p>\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No JSON object could be decoded",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-62-89ca4d06afd3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpost\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mapiurl\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'/audience/estimate'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mjson\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpayload\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Most likely audience size: %d +/- %d (standard error)\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'prediction'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'ste'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"95%% Confidence interval: %d to %d people\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'rangemin'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'rangemax'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/lionfish/anaconda2/lib/python2.7/json/__init__.pyc\u001b[0m in \u001b[0;36mloads\u001b[1;34m(s, encoding, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    337\u001b[0m             \u001b[0mparse_int\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mparse_float\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m \u001b[1;32mand\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    338\u001b[0m             parse_constant is None and object_pairs_hook is None and not kw):\n\u001b[1;32m--> 339\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_default_decoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    340\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    341\u001b[0m         \u001b[0mcls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mJSONDecoder\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/lionfish/anaconda2/lib/python2.7/json/decoder.pyc\u001b[0m in \u001b[0;36mdecode\u001b[1;34m(self, s, _w)\u001b[0m\n\u001b[0;32m    362\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    363\u001b[0m         \"\"\"\n\u001b[1;32m--> 364\u001b[1;33m         \u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraw_decode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0m_w\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    365\u001b[0m         \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_w\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    366\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mend\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/lionfish/anaconda2/lib/python2.7/json/decoder.pyc\u001b[0m in \u001b[0;36mraw_decode\u001b[1;34m(self, s, idx)\u001b[0m\n\u001b[0;32m    380\u001b[0m             \u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscan_once\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    381\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 382\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"No JSON object could be decoded\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    383\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: No JSON object could be decoded"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "data = {\n",
    "\"payload\":'{\"id\":\"58b94e905904d34354359de5\",\"filters\":[{\"cqid\":\"55d1f7adb34bdbd96a1aaa04\",\"comparison\":\"in\",\"values\":[\"female\"]}]}',\n",
    "\"created_at\":\"2017-03-03 11:08:00\"}\n",
    "\n",
    "#put it into the payload of the request. This also includes the version, the api key we're using and the action we want (in this case generate a question)\n",
    "payload = {\"version\":1, 'data': data, 'apikey': 'YOUR_API_KEY_HERE'}\n",
    "r = requests.post(apiurl+'/audience/estimate',json=payload)\n",
    "print r.content\n",
    "response = json.loads(r.content)\n",
    "print(\"Most likely audience size: %d +/- %d (standard error)\" % (int(response['prediction']), int(response['ste'])))\n",
    "print(\"95%% Confidence interval: %d to %d people\" % (int(response['rangemin']),int(response['rangemax'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "data = {\n",
    "\"payload\":'{\"id\":\"58b94e905904d34354359de5\",\"filters\":[{\"cqid\":\"55d1f7adb34bdbd96a1aaa04\",\"comparison\":\"in\",\"values\":[\"female\"]}]}',\n",
    "\"created_at\":\"2017-03-03 11:08:00\"}\n",
    "\n",
    "#put it into the payload of the request. This also includes the version, the api key we're using and the action we want (in this case generate a question)\n",
    "payload = {\"version\":1, 'data': data, 'apikey': 'YOUR_API_KEY_HERE'}\n",
    "r = requests.post(apiurl+'/audience/estimate',json=payload)\n",
    "response = json.loads(r.content)\n",
    "print(\"Most likely audience size: %d +/- %d (standard error)\" % (int(response['prediction']), int(response['ste'])))\n",
    "print(\"95%% Confidence interval: %d to %d people\" % (int(response['rangemin']),int(response['rangemax'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n",
      "Most likely audience size: 29 +/- 17 (standard error)\n",
      "95% Confidence interval: 0 to 64 people\n"
     ]
    }
   ],
   "source": [
    "data = { \"payload\": '{\"id\": \"58b94e905904d34354359de5\", \"filters\": [{\"cqid\": \"55d1f7adb34bdbd96a1aaa04\",\"comparison\": \"in\",\"values\": [\"female\"]}]}',\"created_at\": \"2017-03-03 11:08:00\"}\n",
    "\n",
    "#put it into the payload of the request. This also includes the version, the api key we're using and the action we want (in this case generate a question)\n",
    "payload = {\"version\":1, 'data': data, 'apikey': 'YOUR_API_KEY_HERE'}\n",
    "r = requests.post(apiurl+'/audience/estimate',json=payload)\n",
    "print r\n",
    "response = json.loads(r.content)\n",
    "print(\"Most likely audience size: %d +/- %d (standard error)\" % (int(response['prediction']), int(response['ste'])))\n",
    "print(\"95%% Confidence interval: %d to %d people\" % (int(response['rangemin']),int(response['rangemax'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Get server API version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"version\": \"9.0\"}'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "r = requests.post(apiurl+'/version') #post or get works for this.\n",
    "r.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Typical API usage\n",
    "\n",
    "The scikic front end may use the API in the following way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'dataset': u'geoloc', u'detail': u'{}', u'dataitem': u'country'}\n",
      "Which country are you in now?us\n",
      "{u'dataset': u'postal', u'detail': u'', u'dataitem': u'zipcode'}\n",
      "What's your zip code?12345\n",
      "{u'dataset': u'geoloc', u'detail': u'{}', u'dataitem': u'city'}\n",
      "Which city or town are you in or near now?Tuscon\n",
      "{u'dataset': u'movielens', u'detail': 1200, u'dataitem': u'seen'}\n",
      "Have you seen Aliens (1986)? (yes or no)no\n",
      "{u'facts': {u'guess_loc': {}, u'where': {u'country': [{u'item': u'us', u'probability': 1.0}], u'city': [{u'item': [u'Tuscon', u'us'], u'probability': 1.0}]}, u'where_history': {u'error': u'no_fb_likes'}}, u'relationships': [{u'parent': u'factor_gender', u'child': u'movie_seen_Aliens (1986)'}, {u'parent': u'factor_age', u'child': u'movie_seen_Aliens (1986)'}, {u'parent': u'factor_age', u'child': u'blockgroup'}], u'feature_descriptions': {u'movie_seen_Aliens (1986)': {u'desc': u'Probability of being in this blockgroup given other features'}, u'item6_blockgroup': {u'desc': u'Probability of being in this block group given your features'}, u'blockgroup': {u'desc': u'Your geographical location'}, u'factor_gender': {u'desc': u'Your gender'}, u'factor_age': {u'desc': u'Your age'}}, u'features': {u'movie_seen_Aliens (1986)': {u'distribution': [1], u'quartiles': {u'upper': 0, u'lower': 0, u'mean': 0.0}}, u'item6_blockgroup': {u'distribution': [1], u'quartiles': {u'upper': 0, u'lower': 0, u'mean': 0.0}}, u'blockgroup': {u'distribution': [1.0], u'quartiles': {u'upper': 0, u'lower': 0, u'mean': 0.0}}, u'factor_gender': {u'distribution': [0.45714285714285713, 0.5428571428571428], u'quartiles': {u'upper': 1, u'lower': 0, u'mean': 0.5428571428571428}}, u'factor_age': {u'distribution': [0.005033829744757828, 0.005808265090105185, 0.0059373376476630785, 0.006582700435452544, 0.007228063223242009, 0.007615280895915688, 0.008260643683705153, 0.008389716241263046, 0.008906006471494619, 0.00980951437439987, 0.010325804604631443, 0.01071302227730512, 0.011874675295326159, 0.012132820410441945, 0.012778183198231409, 0.013165400870905088, 0.013036328313347195, 0.012520038083115624, 0.012520038083115623, 0.012649110640673516, 0.012520038083115623, 0.012520038083115623, 0.012261892967999836, 0.01277818319823141, 0.013165400870905088, 0.013165400870905088, 0.013165400870905088, 0.013294473428462981, 0.013165400870905088, 0.013165400870905088, 0.013294473428462981, 0.013552618543578768, 0.013036328313347195, 0.013036328313347195, 0.012778183198231409, 0.012778183198231409, 0.01239096552555773, 0.011874675295326159, 0.011874675295326157, 0.01148745762265248, 0.01071302227730512, 0.01019673204707355, 0.010067659489515657, 0.009293224144168298, 0.009035079029052512, 0.008647861356378832, 0.008260643683705153, 0.008647861356378832, 0.008647861356378832, 0.008647861356378832, 0.00851878879882094, 0.00851878879882094, 0.008389716241263046, 0.008389716241263046, 0.007615280895915689, 0.007098990665684117, 0.006582700435452545, 0.006453627877894652, 0.006711772993010437, 0.0069699181081262235, 0.007098990665684117, 0.007098990665684117, 0.006711772993010438, 0.006582700435452545, 0.006453627877894652, 0.006324555320336759, 0.006195482762778866, 0.006324555320336759, 0.006453627877894652, 0.00684084555056833, 0.007357135780799903, 0.007615280895915689, 0.00813157112614726, 0.00813157112614726, 0.008002498568589367, 0.00813157112614726, 0.008260643683705153, 0.008647861356378832, 0.008389716241263046, 0.008260643683705153, 0.008002498568589367, 0.008002498568589369, 0.008389716241263046, 0.008260643683705153, 0.00813157112614726, 0.008260643683705153, 0.00813157112614726, 0.008131571126147262, 0.008002498568589369, 0.007486208338357796, 0.007098990665684117, 0.006582700435452545, 0.006453627877894652, 0.006324555320336759, 0.006195482762778866, 0.006195482762778866, 0.005937337647663079, 0.005421047417431507, 0.005291974859873615, 0.005033829744757828, 0.004517539514526256], u'quartiles': {u'upper': 73, u'lower': 21, u'mean': 44.489795918367356}}}, u'insights': {u'uscensus_households_list': [76507230, 56624086, 19883144, 5276790, 14606354, 38254129, 31326617, 6927512], u'uscensus_languages': u'Languages spoken in your area include English, Spanish, French, French Creole, Italian, Portuguese or Portuguese Creole, German, Yiddish, a Scandinavian language, Greek, Russian, Polish, Serbo-Croatian, Armenian, Persian, Gujarati, Hindi, Urdu, Chinese, Japanese, Korean, Mon-Khmer, Cambodian, Hmong, Thai, Laotian, Vietnamese, Tagalog, Navajo, Hungarian, Arabic, Hebrew and an African language', u'debug_total_inference_time': u'1824.084044ms', u'uscensus_language_list': [228216716, 36170544, 1320191, 688675, 747439, 682323, 1102500, 154939, 132959, 311136, 851367, 598130, 264723, 230450, 374457, 348796, 606174, 372994, 2723002, 455107, 1122760, 206648, 204745, 154551, 153469, 1330171, 1566544, 168146, 90976, 810169, 206969, 798306], u'uscensus_genderratio': u'There are 109% more women than men aged 101 to 106 living in your area.', u'like_locations_debug': u\"Note: I can't see your facebook likes.\", u'uscensus_birthplace': u'59% of people in your neighbourhood were born in your state.', u'uscensus_popage': u'Half the people in your neighbourhood are younger than 36 years old.', u'uscensus_debug_languages': u'[[228216716, 36170544, 1320191, 688675, 747439, 682323, 1102500, 154939, 132959, 311136, 851367, 598130, 264723, 230450, 374457, 348796, 606174, 372994, 2723002, 455107, 1122760, 206648, 204745, 154551, 153469, 1330171, 1566544, 168146, 90976, 810169, 206969, 798306]]', u'uscensus_birthplace_list': [179872457, 19882909, 22682742, 26156507, 14588037, 39268838]}}\n",
      "\n",
      "Insights\n",
      "\n",
      "uscensus_households_list\n",
      "[76507230, 56624086, 19883144, 5276790, 14606354, 38254129, 31326617, 6927512]\n",
      "uscensus_languages\n",
      "Languages spoken in your area include English, Spanish, French, French Creole, Italian, Portuguese or Portuguese Creole, German, Yiddish, a Scandinavian language, Greek, Russian, Polish, Serbo-Croatian, Armenian, Persian, Gujarati, Hindi, Urdu, Chinese, Japanese, Korean, Mon-Khmer, Cambodian, Hmong, Thai, Laotian, Vietnamese, Tagalog, Navajo, Hungarian, Arabic, Hebrew and an African language\n",
      "debug_total_inference_time\n",
      "1824.084044ms\n",
      "uscensus_language_list\n",
      "[228216716, 36170544, 1320191, 688675, 747439, 682323, 1102500, 154939, 132959, 311136, 851367, 598130, 264723, 230450, 374457, 348796, 606174, 372994, 2723002, 455107, 1122760, 206648, 204745, 154551, 153469, 1330171, 1566544, 168146, 90976, 810169, 206969, 798306]\n",
      "uscensus_genderratio\n",
      "There are 109% more women than men aged 101 to 106 living in your area.\n",
      "like_locations_debug\n",
      "Note: I can't see your facebook likes.\n",
      "uscensus_birthplace\n",
      "59% of people in your neighbourhood were born in your state.\n",
      "uscensus_popage\n",
      "Half the people in your neighbourhood are younger than 36 years old.\n",
      "uscensus_debug_languages\n",
      "[[228216716, 36170544, 1320191, 688675, 747439, 682323, 1102500, 154939, 132959, 311136, 851367, 598130, 264723, 230450, 374457, 348796, 606174, 372994, 2723002, 455107, 1122760, 206648, 204745, 154551, 153469, 1330171, 1566544, 168146, 90976, 810169, 206969, 798306]]\n",
      "uscensus_birthplace_list\n",
      "[179872457, 19882909, 22682742, 26156507, 14588037, 39268838]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "#We start with no questions asked, none unprocessed, and nothing in the facts dictionary.\n",
    "questions_asked = []\n",
    "unprocessed_questions = []\n",
    "facts = {}\n",
    "\n",
    "for loop in range(4): #we'll ask four questions\n",
    "    \n",
    "    #1. get the question (populate the data dictionary & send it off)\n",
    "    data = {'unprocessed_questions':unprocessed_questions,'questions_asked':questions_asked,'facts':facts}\n",
    "    payload = {\"version\":1, 'data': data, 'apikey': 'YOUR_API_KEY_HERE'}\n",
    "    r = requests.post(apiurl+'/question',json=payload) #>>>\n",
    "    question_query_result = json.loads(r.content)\n",
    "    \n",
    "    #if processing was done then more items will be available for 'facts':\n",
    "    facts = question_query_result['facts']\n",
    "    question = question_query_result['question']\n",
    "    print question\n",
    "    #ask the user this question    \n",
    "    userinput = raw_input(question_query_result['question_string']['question'])\n",
    "    question['answer'] = userinput #add their answer\n",
    "\n",
    "    #add this to the list of questions we've asked, and unprocessed questions\n",
    "    questions_asked.append(question)\n",
    "    unprocessed_questions = [question] #.append(question)\n",
    "    \n",
    "#3. Once enough questions are asked we can do inference.\n",
    "#   Populate the data dictionary with questions asked,\n",
    "#   unprocessed questions and facts (as for the question query in step 1)\n",
    "data = {'questions_asked':questions_asked,'unprocessed_questions':unprocessed_questions,'facts':facts}\n",
    "payload = {\"version\":1, 'data': data, 'apikey': 'YOUR_API_KEY_HERE'}\n",
    "r = requests.post(apiurl+'/inference',json=payload)\n",
    "inference_results = json.loads(r.content)\n",
    "\n",
    "print inference_results\n",
    "#It generates insights from these questions, which are displayed below.\n",
    "print \"\\nInsights\\n\"\n",
    "for insight in inference_results['insights']:\n",
    "    print insight\n",
    "    print inference_results['insights'][insight]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
