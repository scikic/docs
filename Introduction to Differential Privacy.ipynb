{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Introduction\n",
      "\n",
      "> differential privacy: \"maximize the accuracy of queries from statistical databases while minimizing the chances of identifying its records.\" (wikipedia)\n",
      "\n",
      "> differential privacy: \"a promise by a curator to a data subject that they will not be affected by being included, regardless what other sources of data are available\" (privacy book, The Algorithmic Foundations of Differential Privacy - Cynthia Dwork & Aaron Roth).\n",
      "\n",
      "This brings in the idea of a \"linkage attack\", combining data sources to infer details about individuals in an \"anonymised\" dataset.\n",
      "\n",
      "From Dwork & Roth again:\n",
      "> \"[DP]... ensures that any [output] is \u201cessentially\u201d equally likely to occur, independent of the presence or absence of any\n",
      "individual. The term \u201cessentially\u201d is captured by a parameter, $\\varepsilon$. A smaller $\\varepsilon$ will yield better privacy (and less accurate responses).\n",
      "\n",
      "And a list of hints/rules-of-thumb (again from Dwork & Roth, actually most of this document is based on their book or wikipedia, which is also based on their book. Maybe just go read the book!).\n",
      "\n",
      " - Data cannot be fully anonymised and remain useful.\n",
      " - Reidentification of anonymised records is not the only risk.\n",
      " - Queries over large sets are not protective.\n",
      " - Query auditing is problematic (e.g. refusing a query could be disclosive, computationally infeasible).\n",
      " - Summary statistics are not safe.\n",
      " - \"Ordinary facts\" are not ok (e.g. shopping habits - give clues around health, household, religion, etc).\n",
      " - Cancel privacy for \"just a few\" people, for \"greater good\" (outliers, etc): A possible moral standpoint, but which is usually (reasonably) rejected.\n",
      "\n",
      "A widely used example of Differential Privacy (before it was called that), is the coin-flip example. Participants being asked if they take part in illegal or socially-unacceptable behaviour will often lie. Sociologists have avoided this problem by asking people to flip a coin: If tails they should report the truth, but if heads, the participant must flip again and report yes if heads, no if tails.\n",
      "\n",
      "![alt text](files/images/diff_priv_coin_small.png \"Coin Flip\")\n",
      "\n",
      "Effectively provides 'plausible deniability'.\n",
      "\n",
      "***Any nontrivial privacy guarantee that holds, regardless of all present and future sources of auxiliary information requires RANDOMISATION***\n",
      "\n",
      "###A simple example\n",
      "\n",
      "Using the wikipedia example, and extending/explaining it.\n",
      "\n",
      "If we have a database, such as this one:\n",
      "    \n",
      "    Name    Has Diabetes\n",
      "    0 Adam         0\n",
      "    1 Beth         1\n",
      "    2 Charlie      1\n",
      "    3 Diane        0\n",
      "    4 Elizabeth    0\n",
      "    5 Fred         1 (new person)\n",
      "\n",
      "We want to provide a summary statistic (such as the sum number of people with diabetes: Maybe to know how many drugs to stock).\n",
      "\n",
      "If a new person is added to the database, we could query it again. Unfortunately by subtracting the two values we could find his diabetes-state, even though the query result itself doesn't mention names.\n",
      "\n",
      "Differential Privacy aims to avoid that. Before releasing the summary value, one must apply a function to the value to provide this privacy. This is usually a matter of adding a random value. For this example, if we are first asked for the sum (without Fred), we take the actual sum (2) and add/subtract a value (e.g. +1.6): Giving 3.6 as the sum. \n",
      "\n",
      "When Fred is added and someone asked for the sum, we do the same: Take the actual sum (3) and add/subtract a random value (e.g. +0.9), giving 3.9.\n",
      "\n",
      "The adversary, who asked these two queries can't be sure whether Fred has diabetes.\n",
      "\n",
      "###More formal definition\n",
      "\n",
      "Slightly more formally:\n",
      "\n",
      "We have a randomisation algorithm $M$. It is $\\varepsilon$-private if the following holds:\n",
      "\n",
      "If we consider the outputs $M(D_1)$ and $M(D_2)$ provided to the adversary, from the database with ($D_1$) and without ($D_2$) Fred, we can compare the probability of any given value output $P[M(D_1)]$ and $P[M(D_2)]$. We want the probabilities of these two outputs to be very similar, such that, for any value of M:\n",
      "\n",
      "$P[M(D_1)] \\leq e^{\\varepsilon} P[M(D_2)] + \\delta$\n",
      "\n",
      "Note: Often the two databases are defined by their $L_1$ norm, such that: $||D_1 - D_2|| \\leq 1$. Where ||D|| is the sum over the number of elements of each type. Basically the length of the database (?). **TBC**\n",
      "\n",
      "####Delta: $\\delta$\n",
      "\n",
      "If $\\delta=0$ we say that the algorithm is $\\varepsilon$-differentially private.\n",
      "\n",
      "For example,...? TODO...\n",
      "\n",
      "###Coin flip example continued...\n",
      "\n",
      "From http://www.cs.cmu.edu/afs/cs/academic/class/15859m-s11/www/lectures/lect0420.pdf\n",
      "\n",
      "It is possibly clearer to write the above inequality as:\n",
      "\n",
      "$e^{-\\varepsilon} \\leq {{P[M(X)=v]} \\over {P[M(X')=v]}} \\leq e^{\\varepsilon}$\n",
      "\n",
      "where $X$ and $X'$ are the two databases, with one row changed, where the change is the one with the largest possible effective on the value of $M(X)$.\n",
      "\n",
      "The coin flip above, the probability of an outcome (e.g. a Yes) given the underlying 'database'/actual value is 'True', is ${}^3/_4$. Similarly the probability of the same outcome given the most different underlying actual value (False) is, ${}^1/_4$. Substituting into the above equation: \n",
      "\n",
      "$e^{-\\varepsilon} \\leq {{3/4} \\over {1/4}} \\leq e^{\\varepsilon}$\n",
      "\n",
      "$3 = e^{\\varepsilon}$\n",
      "\n",
      "$\\varepsilon = ln(3)$\n",
      "\n",
      "So we can say that this algorithm is $(ln\\; 3, 0)$-differentially private."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "####Gaussian output perterbation and Laplace mechanism\n",
      "\n",
      "The obvious general solution is to add some gaussian noise to the output. Consider $n$ rows, with a column of values between 0 and $\\Delta f$, where we set $\\Delta f$ as 40k for this example. Note: $\\Delta f$ is known, in general as the $l_1$-sensitivity of the function, and it's how different two database outputs could be, given the databases are neighbours.\n",
      "    \n",
      "    Person  Income/\u00a3k\n",
      "    Amy       20\n",
      "    Bob       17\n",
      "    Charlie   23\n",
      "    Diana     37\n",
      "    Elsbeth   26\n",
      "    Fred      22\n",
      "    Gavin     17\n",
      "    Holly     29\n",
      "\n",
      "We want to find the total income. The largest an individual might change the mean is 40k. We want to add sufficient noise to sufficiently mask this change. If we first consider adding Gaussian noise, then the ratio of the two probabilities is:\n",
      "\n",
      "${exp[{{{-x^2} \\over {2\\sigma^2}}}]\n",
      "\\over\n",
      "{exp[{{-(x-[\\Delta f])^2} \\over {2\\sigma^2}}]}}$\n",
      "\n",
      "which can be rearranged and cancelled through:\n",
      "\n",
      "${exp[{{{-x^2} \\over {2\\sigma^2}}}]\n",
      "\\over\n",
      "{exp[{{-(x-[\\Delta f])^2} \\over {2\\sigma^2}}]}}$\n",
      "\n",
      "$exp[{{-x^2 + (x-[b])^2} \\over {2 \\sigma^2}}] = exp[{{-2(\\Delta f)x+{\\Delta f}^2} \\over {2 \\sigma^2}}]$\n",
      "\n",
      "$\\Delta f^2$ is almost zero, so ignoring that term:\n",
      "\n",
      "$e^-\\varepsilon = exp[{{-2(\\Delta f)x} \\over {2 \\sigma^2}}]$\n",
      "\n",
      "$\\varepsilon = {2\\Delta f x \\over {2 \\sigma^2}}$\n",
      "\n",
      "Extra $x$ might be a problem?? Use Laplace instead!"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "####Laplace Mechanism\n",
      "\n",
      "$exp[-\\varepsilon] = {{exp[|x|/b}] \\over {exp[|x-{\\Delta f}|\\;/\\;b]}}$\n",
      "\n",
      "$exp[-\\varepsilon] = exp[{-{\\Delta f} \\over {b}}]$\n",
      "\n",
      "$\\varepsilon = {{\\Delta f} \\over {b}}$\n",
      "\n",
      "So we set the parameter $b$ equal to ${\\Delta f} / {\\varepsilon}$.\n",
      "\n",
      "So in our example from earlier, $\\Delta f=40$. We finally need to select a value for $\\varepsilon$ which balances the privacy of the members of database and the utility researchers can gain from the database (a smaller value gives more privacy). In this example let's set $\\varepsilon$ to 1: $b = 40$.\n",
      "\n",
      "####Privacy loss\n",
      "[to complete]\n",
      "How much privacy is lost? The log of the probability ratio is known as the *privacy loss*, incurred by observing a result, E.\n",
      "\n",
      "$L^{(E)}_{M(x)||M(x')} = ln \\left( {{P[M(x)=E]} \\over {P[M(x')=E]}} \\right)$\n",
      "\n",
      "The absolute value of the privacy loss is bounded by $\\varepsilon$ (with probability $1-\\delta$).\n",
      "\n",
      "###Theorems (summarised into English a bit)\n",
      "\n",
      "- If several queries are made $(\\varepsilon_1,\\delta_1)$, $(\\varepsilon_2,\\delta_2)$,... then the $\\varepsilon$s and $\\delta$s add up.\n",
      "\n",
      "- If several rows are of concern (\"group privacy\"), for query of $(\\varepsilon,0)$-DP the same query is only $(k\\varepsilon,0)$-DP for $k$ rows."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "####Counting Queries...\n",
      "\n",
      "\"How many people have an income over \u00a325?\"\n",
      "\n",
      "The sensitivity of a counting query, $\\Delta f = 1$ (as each row can only change the result by a maximum of 1. We therefore simply add Laplace noise with parameter $b = 1/\\varepsilon$. If $\\varepsilon=1$, then $b=1$."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "income = np.array([20,17,23,37,26,22,17,29])\n",
      "np.sum(income>25)+np.random.laplace(0,1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 5,
       "text": [
        "2.3767712043304892"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "If 5 counting queries are asked, we need to use $\\varepsilon_x = \\varepsilon / 5$ for each query to ensure $\\varepsilon$ privacy overall. $b = \\Delta f / \\varepsilon_x = 1/5$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "income = np.array([20,17,23,37,26,22,17,29])\n",
      "print np.sum(income>35)+np.random.laplace(0,5)\n",
      "print np.sum((income>25) & (income<=35))+np.random.laplace(0,5)\n",
      "print np.sum((income>15) & (income<=25))+np.random.laplace(0,5)\n",
      "print np.sum((income>5) & (income<=15))+np.random.laplace(0,5)\n",
      "print np.sum(income<=5)+np.random.laplace(0,5)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "-1.7240892506\n",
        "-6.84852639568\n",
        "15.1490919063\n",
        "0.87224599347\n",
        "8.83005847491\n"
       ]
      }
     ],
     "prompt_number": 28
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "np.sum(income)+np.random.laplace(0,40)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 8,
       "text": [
        "169.78594073894317"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "####Histogram Queries...\n",
      "Disjoint cells: Can be more efficient than just a series of seperate counts as the addition of one row can only affect one cell. Means, as long as the 5 cells are disjoint we can just use $\\varepsilon$.\n",
      "\n",
      "####Report noisy max\n",
      "To do"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Applying to individual collections of data\n",
      "\n",
      "To rephrase the above to our problem: What effect does adding a data point $x$ have to the summary value? We need to know the sensitivity of the algorithm. Questions come up about normalisation (to normalise we need to know the values of the variables!).\n",
      "\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}